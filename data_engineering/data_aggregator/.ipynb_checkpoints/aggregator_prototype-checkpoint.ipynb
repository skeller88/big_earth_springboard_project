{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json \n",
    "from hashlib import sha256\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/jovyan/work'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import glob\n",
    "\n",
    "logger = logging.getLogger()\n",
    "json_files_dir = root + \"/data/big_earth/BigEarthNet-V1.0\"\n",
    "csv_output_dir = root + \"/data/big_earth/metadata\"\n",
    "\n",
    "logger.info('test')\n",
    "\n",
    "if not os.path.exists(csv_output_dir):\n",
    "    os.mkdir(csv_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from hashlib import sha256\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from concurrent.futures import Future\n",
    "from concurrent.futures.thread import ThreadPoolExecutor\n",
    "from typing import List\n",
    "    \n",
    "    \n",
    "def parallelize_task(num_workers, iterator, task, **task_kwargs):\n",
    "    chunk_size = len(iterator) // num_workers\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        tasks: List[Future] = []\n",
    "        start_index = 0\n",
    "        for x in range(num_workers):\n",
    "            end_index = min(start_index + chunk_size + 1, len(iterator))\n",
    "            chunk = iterator[start_index:end_index]\n",
    "            tasks.append(executor.submit(task, chunk, **task_kwargs))\n",
    "            start_index = end_index\n",
    "\n",
    "        return [task.result() for task in tasks]\n",
    "\n",
    "\n",
    "logger = logging.Logger(\"archive_etler\", level=logging.INFO)\n",
    "csv_files_path=csv_output_dir\n",
    "cloud_and_snow_csv_dir=root + \"/data/big_earth\"\n",
    "json_dir=json_files_dir               \n",
    "\n",
    "\n",
    "if not os.path.exists(csv_files_path):\n",
    "    os.mkdir(csv_files_path)\n",
    "\n",
    "# From BigEarth team: we used the same labels of the CORINE Land Cover program operated by the European Environment\n",
    "# Agency. You can check the label names from\n",
    "# https://land.copernicus.eu/user-corner/technical-library/corine-land-cover-nomenclature-guidelines/html/.\n",
    "replacements = {\n",
    "    'Bare rocks': 'Bare rock',\n",
    "    'Natural grasslands': 'Natural grassland',\n",
    "    'Peat bogs': 'Peatbogs',\n",
    "    'Transitional woodland-shrub': 'Transitional woodland/shrub'\n",
    "}\n",
    "\n",
    "def multi_replace(arr):\n",
    "    return [replacements[el] if replacements.get(el) is not None else el for el in arr]\n",
    "\n",
    "def read_and_augment_metadata(json_metadata_file, mlb):\n",
    "    with open(json_metadata_file) as fileobj:\n",
    "        obj = json.load(fileobj)\n",
    "        obj['labels'] = multi_replace(obj['labels'])\n",
    "        obj['labels_sha256_hexdigest'] = sha256('-'.join(obj['labels']).encode('utf-8')).hexdigest()\n",
    "        obj['binarized_labels'] = mlb.transform([obj['labels']])\n",
    "        obj['image_prefix'] = json_metadata_file.rsplit('/')[-2]\n",
    "        return obj\n",
    "\n",
    "def json_metadata_from_files(json_metadata_files, mlb):\n",
    "    return [read_and_augment_metadata(json_metadata_file, mlb) for json_metadata_file in json_metadata_files]\n",
    "\n",
    "start = time.time()\n",
    "# glob_path = json_dir + '/**/*.json'\n",
    "# paths = glob.glob(glob_path)\n",
    "imgs = [\"S2A_MSIL2A_20170613T101031_0_{}\".format(num) for num in range(45, 88)]\n",
    "paths = [json_dir + f'/{image}/{image}_labels_metadata.json' for image in imgs]\n",
    "logger.info(f\"Fetched {len(paths)} paths. in {time.time() - start} seconds.\")\n",
    "start = time.time()\n",
    "\n",
    "# 44 level 3 classes:\n",
    "# Currently using:\n",
    "# https://land.copernicus.eu/user-corner/technical-library/corine-land-cover-nomenclature-guidelines/html/\n",
    "classes = [\"Continuous urban fabric\", \"Discontinuous urban fabric\", \"Industrial or commercial units\",\n",
    "       \"Road and rail networks and associated land\", \"Port areas\", \"Airports\", \"Mineral extraction sites\",\n",
    "       \"Dump sites\",\n",
    "       \"Construction sites\", \"Green urban areas\", \"Sport and leisure facilities\", \"Non-irrigated arable land\",\n",
    "       \"Permanently irrigated land\", \"Rice fields\", \"Vineyards\", \"Fruit trees and berry plantations\",\n",
    "       \"Olive groves\",\n",
    "       \"Pastures\", \"Annual crops associated with permanent crops\", \"Complex cultivation patterns\",\n",
    "       \"Land principally occupied by agriculture, with significant areas of natural vegetation\",\n",
    "       \"Agro-forestry areas\",\n",
    "       \"Broad-leaved forest\", \"Coniferous forest\", \"Mixed forest\", \"Natural grassland\", \"Moors and heathland\",\n",
    "       \"Sclerophyllous vegetation\", \"Transitional woodland/shrub\", \"Beaches, dunes, sands\", \"Bare rock\",\n",
    "       \"Sparsely vegetated areas\", \"Burnt areas\", \"Glaciers and perpetual snow\", \"Inland marshes\", \"Peatbogs\",\n",
    "       \"Salt marshes\", \"Salines\", \"Intertidal flats\", \"Water courses\", \"Water bodies\", \"Coastal lagoons\",\n",
    "       \"Estuaries\",\n",
    "       \"Sea and ocean\"]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([classes])\n",
    "# sanity check the output\n",
    "logger.info(f\"Sea and ocean: {mlb.transform([['Sea and ocean']])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imageio: 'libfreeimage-3.16.0-linux64.so' was not found on your computer; downloading it now.\n",
      "Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/freeimage/libfreeimage-3.16.0-linux64.so (4.6 MB)\n",
      "Downloading: 8192/4830080 bytes (0.2%999424/4830080 bytes (20.71507328/4830080 bytes (31.2%2015232/4830080 bytes (41.7%2506752/4830080 bytes (51.9%2973696/4830080 bytes (61.6%3489792/4830080 bytes (72.3%3973120/4830080 bytes (82.3%4464640/4830080 bytes (92.4%4830080/4830080 bytes (100.0%)\n",
      "  Done\n",
      "File saved as /home/jovyan/.imageio/freeimage/libfreeimage-3.16.0-linux64.so.\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "imageio.plugins.freeimage.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 120, 3) uint16\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "\n",
    "img_dir = root + \"/data/S2A_MSIL2A_20170613T101031_6_59\"\n",
    "\n",
    "def image_files_from_tif_to_npy(num_workers, npy_files_path, image_dir, image_prefixes):\n",
    "    if not os.path.exists(npy_files_path):\n",
    "        os.mkdir(npy_files_path)\n",
    "\n",
    "    def image_to_rgb_tiff(image_prefix):\n",
    "        # cv2 expects BGR format\n",
    "        bands = [cv2.imread(f\"{image_dir}/{image_prefix}/{image_prefix}_B{band}.tif\") \n",
    "                 for band in [\"04\", \"03\", \"02\"]]\n",
    "\n",
    "        stacked_arr = np.stack(bands, axis=-1)\n",
    "        return cv2.fromarray(stacked_arr)\n",
    "#         np.save(f\"{npy_files_path}/{image_prefix}\", stacked_arr)\n",
    "\n",
    "    def images_to_npy(image_prefixes):\n",
    "        for image_prefix in image_prefixes:\n",
    "            image_to_rgb_tiff(image_prefix)\n",
    "            \n",
    "# image_files_to_tiff_file(10, 'foo', root + \"/data\", [\"S2A_MSIL2A_20170613T101031_6_59\"])\n",
    "\n",
    "def image_to_rgb_tiff(image_prefix):    \n",
    "    bands = [np.asarray(\n",
    "        Image.open(f\"{root}/data/{image_prefix}/{image_prefix}_B{band}.tif\"),\n",
    "        dtype=np.uint16) for band in [\"04\", \"03\", \"02\"]]\n",
    "    \n",
    "    stacked_arr = np.stack(bands, axis=-1)\n",
    "    return stacked_arr\n",
    "#     cv2.imwrite('test.png', stacked_arr, \"flag\")\n",
    "\n",
    "arr = image_to_rgb_tiff(\"S2A_MSIL2A_20170613T101031_6_59\")\n",
    "print(arr.shape, arr.dtype)\n",
    "# https://github.com/imageio/imageio/issues/146\n",
    "imageio.imwrite(im=arr, uri='test.png', format='PNG-FI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test.png'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.abspath('test.png').rsplit('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(arr == imageio.imread('test.png', format='PNG-FI')).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sha1:e2f5b7048657:86d888ae3239ec89ca20e2fc3d0c9dca2f9ef38c'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.lib.security import passwd\n",
    "passwd(\"Take me to your river succulent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  has_snow\n",
      "image_prefix                              \n",
      "S2B_MSIL2A_20170831T095029_27_76         1\n",
      "S2B_MSIL2A_20170831T095029_27_85         1\n",
      "S2B_MSIL2A_20170831T095029_29_75         1\n",
      "\n",
      "\n",
      "                                  has_cloud_and_shadow\n",
      "image_prefix                                          \n",
      "S2A_MSIL2A_20170717T113321_35_89                     1\n",
      "S2A_MSIL2A_20170717T113321_39_84                     1\n",
      "S2B_MSIL2A_20171112T114339_0_13                      1\n"
     ]
    }
   ],
   "source": [
    "json_object_lists = parallelize_task(num_workers=20, iterator=paths, task=json_metadata_from_files, **dict(mlb=mlb))\n",
    "df = pd.concat([pd.DataFrame.from_records(json_object_list) for json_object_list in json_object_lists])\n",
    "# Check the dimensions\n",
    "logger.info(f\"len(df): {len(df)}, len(paths): {len(paths)}\")\n",
    "logger.info(f\"Read files into dataframe in {time.time() - start} seconds.\")\n",
    "\n",
    "# Denote if patch has snow and/or cloudsrandom_state\n",
    "snow = pd.read_csv(os.path.join(cloud_and_snow_csv_dir, 'patches_with_seasonal_snow.csv'), header=None, names=['image_prefix'])\n",
    "snow_col = 'has_snow'\n",
    "snow[snow_col] = 1\n",
    "snow = snow.set_index('image_prefix')\n",
    "\n",
    "clouds = pd.read_csv(os.path.join(cloud_and_snow_csv_dir, 'patches_with_cloud_and_shadow.csv'), header=None, names=['image_prefix'])\n",
    "cloud_col = 'has_cloud_and_shadow'\n",
    "clouds[cloud_col] = 1\n",
    "clouds = clouds.set_index('image_prefix')\n",
    "\n",
    "print(snow.head(3))\n",
    "len_snow = len(snow)\n",
    "print('\\n')\n",
    "print(clouds.head(3))\n",
    "len_clouds = len(clouds)\n",
    "\n",
    "for column in [snow_col, cloud_col]:\n",
    "    df[column] = 0\n",
    "\n",
    "df = df.set_index('image_prefix')\n",
    "df.update(snow)\n",
    "df.update(clouds)\n",
    "# assert df[snow_col].sum() == len_snow\n",
    "# assert df[cloud_col].sum() == len_clouds\n",
    "\n",
    "df.to_csv(csv_files_path + '/metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "npy_files_path = f\"{root}/data/big_earth/npy_files\"\n",
    "if os.path.exists(npy_files_path):\n",
    "    os.rmdir(npy_files_path, recursive=True)\n",
    "os.mkdir(npy_files_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'/home/jovyan/work/data/big_earth/metadatametadata.csv' does not exist: b'/home/jovyan/work/data/big_earth/metadatametadata.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-19b775046faf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_output_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'metadata.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdataset_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msample_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfrac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_size\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdataset_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/home/jovyan/work/data/big_earth/metadatametadata.csv' does not exist: b'/home/jovyan/work/data/big_earth/metadatametadata.csv'"
     ]
    }
   ],
   "source": [
    "metadata = pd.read_csv(csv_output_dir + 'metadata.csv')\n",
    "dataset_size = len(metadata)\n",
    "sample_size = 20000\n",
    "frac = sample_size / dataset_size\n",
    "sample = metadata.sample(frac=frac, random_state=0)\n",
    "print(len(sample))\n",
    "\n",
    "def image_files_to_npy_file(image_prefix):\n",
    "    bands = [np.asarray(\n",
    "    Image.open(f\"{root}/data/big_earth/BigEarthNet-v1.0/{image_prefix}/{image_prefix}_B{band}.tif\"),\n",
    "    dtype=np.uint16) for band in [\"02\", \"03\", \"04\"]]\n",
    "    \n",
    "    stacked_arr = np.stack(bands, axis=-1)\n",
    "    np.save(f\"{npy_files_path}/{image_prefix}\", stacked_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "db.from_sequence(sample['image_prefix'].values, npartitions=50).map(image_files_to_npy_file).compute()\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "\n",
    "class AugmentedImageSequence(Sequence):\n",
    "    def __init__(self, x: np.array, y: np.array, batch_size, augmentations):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.base_index = [idx for idx in range(len(x))]\n",
    "        self.batch_size = batch_size\n",
    "        self.augmentations = augmentations\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, batch_num):\n",
    "        if batch_num == 0:\n",
    "            print('getting batch_num', batch_num)\n",
    "            start = time.time()\n",
    "\n",
    "        batch_x = self.x[batch_num * self.batch_size:(batch_num + 1) * self.batch_size]\n",
    "\n",
    "        if self.y is not None:\n",
    "            batch_y = self.y[batch_num * self.batch_size:(batch_num + 1) * self.batch_size]\n",
    "\n",
    "        start = time.time()\n",
    "        images = self.batch_loader(batch_x)\n",
    "\n",
    "        # training\n",
    "        if self.y is not None:\n",
    "            batch_x = np.stack([self.augmentations(image=x)[\"image\"] for x in images], axis=0)\n",
    "\n",
    "            if batch_num == 0:\n",
    "                print('fetched batch_num', batch_num, 'in', time.time() - start, 'seconds')\n",
    "\n",
    "            return batch_x, batch_y\n",
    "        # test (inference only)\n",
    "        else:\n",
    "            return np.array(images)\n",
    "\n",
    "    def batch_loader(self, image_paths) -> np.array:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        shuffled_index = self.base_index.copy()\n",
    "        random.shuffle(shuffled_index)\n",
    "        self.x = self.x[shuffled_index]\n",
    "\n",
    "        if self.y is not None:\n",
    "            self.y = self.y[shuffled_index]\n",
    "\n",
    "class AugmentedImageSequenceFromNpy(AugmentedImageSequence):\n",
    "    def __init__(self, x: np.array, y: np.array, batch_size, augmentations):\n",
    "        super().__init__(x=x, y=y, batch_size=batch_size, augmentations=augmentations)\n",
    "\n",
    "    def batch_loader(self, image_paths) -> np.array:\n",
    "        return np.array([np.load(image_path) for image_path in image_paths])\n",
    "    \n",
    "    \n",
    "class AugmentedImageSequenceFromTiff(AugmentedImageSequence):\n",
    "    def __init__(self, x: np.array, y: np.array, batch_size, augmentations):\n",
    "        super().__init__(x=x, y=y, batch_size=batch_size, augmentations=augmentations)\n",
    "\n",
    "    def batch_loader(self, image_paths) -> np.array:\n",
    "        return np.array([self.load_image_bands_from_disk(image_path) for image_path in image_paths])\n",
    "\n",
    "    def load_image_bands_from_disk(self, base_filename):\n",
    "        bands = []\n",
    "        for band in [\"02\", \"03\", \"04\"]:\n",
    "            bands.append(np.array(Image.open(base_filename.format(band)), dtype=np.uint16))\n",
    "        return np.stack(bands, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    Compose, Flip, VerticalFlip, Resize, Rotate, ToFloat\n",
    ")\n",
    "import time\n",
    "\n",
    "AUGMENTATIONS_TRAIN = Compose([\n",
    "    Flip(p=0.5),\n",
    "    Rotate(limit=(0, 360), p=0.5)\n",
    "])\n",
    "\n",
    "AUGMENTATIONS_TEST = Compose([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_files_path = root + \"/data/big_earth/BigEarthNet-V1.0\"\n",
    "\n",
    "xtrain_npy = (npy_files_path + \"/\" + sample.iloc[:5000]['image_prefix'] + \".npy\").values\n",
    "xtrain_tiff = (tiff_files_path + \"/\" + sample.iloc[:5000]['image_prefix'] + \"/\" +\n",
    "               sample.iloc[:5000]['image_prefix'] + \"_B{}.tif\").values\n",
    "\n",
    "ytrain = np.array([np.random.randn(1, 44) for _ in range(len(xtrain))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "np_sequence = AugmentedImageSequenceFromNpy(x=xtrain_npy, y=ytrain, batch_size=batch_size,\n",
    "                                  augmentations=AUGMENTATIONS_TRAIN)\n",
    "tiff_sequence = AugmentedImageSequenceFromTiff(x=xtrain_tiff, y=ytrain, batch_size=batch_size,\n",
    "                                  augmentations=AUGMENTATIONS_TRAIN)\n",
    "\n",
    "def benchmark(sequence):\n",
    "    start = time.time()\n",
    "    for x, y in sequence:\n",
    "        # simulate training step\n",
    "        time.sleep(0.01)\n",
    "    print(\"finished epoch in\", time.time() - start, \"seconds\")\n",
    "\n",
    "print('np_sequence benchmark')\n",
    "benchmark(np_sequence)\n",
    "\n",
    "print('\\n')\n",
    "print('tiff sequence benchmark')\n",
    "benchmark(tiff_sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

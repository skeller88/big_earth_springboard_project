{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow==0.15.* in /opt/conda/lib/python3.7/site-packages (0.15.1)\r\n",
      "Requirement already satisfied: six>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from pyarrow==0.15.*) (1.13.0)\r\n",
      "Requirement already satisfied: numpy>=1.14 in /opt/conda/lib/python3.7/site-packages (from pyarrow==0.15.*) (1.17.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow==0.15.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: petastorm[opencv,tf_gpu] in /opt/conda/lib/python3.7/site-packages (0.8.0)\n",
      "Requirement already satisfied: albumentations in /opt/conda/lib/python3.7/site-packages (0.4.3)\n",
      "Requirement already satisfied: pyspark>=2.1.0 in /usr/local/spark-2.4.4-bin-hadoop2.7/python (from petastorm[opencv,tf_gpu]) (2.4.4)\n",
      "Requirement already satisfied: psutil>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from petastorm[opencv,tf_gpu]) (5.6.7)\n",
      "Requirement already satisfied: pandas>=0.19.0 in /opt/conda/lib/python3.7/site-packages (from petastorm[opencv,tf_gpu]) (0.25.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from petastorm[opencv,tf_gpu]) (1.17.3)\n",
      "Requirement already satisfied: pyarrow>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from petastorm[opencv,tf_gpu]) (0.15.1)\n",
      "Requirement already satisfied: packaging>=15.0 in /opt/conda/lib/python3.7/site-packages (from petastorm[opencv,tf_gpu]) (19.2)\n",
      "Requirement already satisfied: future>=0.10.2 in /opt/conda/lib/python3.7/site-packages (from petastorm[opencv,tf_gpu]) (0.18.2)\n",
      "Requirement already satisfied: dill>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from petastorm[opencv,tf_gpu]) (0.3.1.1)\n",
      "Requirement already satisfied: diskcache>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from petastorm[opencv,tf_gpu]) (4.1.0)\n",
      "Requirement already satisfied: pyzmq>=14.0.0 in /opt/conda/lib/python3.7/site-packages (from petastorm[opencv,tf_gpu]) (18.1.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from petastorm[opencv,tf_gpu]) (1.13.0)\n",
      "Requirement already satisfied: opencv-python>=3.2.0.6; extra == \"opencv\" in /opt/conda/lib/python3.7/site-packages (from petastorm[opencv,tf_gpu]) (4.1.2.30)\n",
      "Requirement already satisfied: tensorflow-gpu>=1.4.0; extra == \"tf_gpu\" in /opt/conda/lib/python3.7/site-packages (from petastorm[opencv,tf_gpu]) (2.1.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from albumentations) (1.4.1)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from albumentations) (5.1.2)\n",
      "Requirement already satisfied: imgaug<0.2.7,>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from albumentations) (0.2.6)\n",
      "Requirement already satisfied: py4j==0.10.7 in /opt/conda/lib/python3.7/site-packages (from pyspark>=2.1.0->petastorm[opencv,tf_gpu]) (0.10.7)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.19.0->petastorm[opencv,tf_gpu]) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.19.0->petastorm[opencv,tf_gpu]) (2.8.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=15.0->petastorm[opencv,tf_gpu]) (2.4.5)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu>=1.4.0; extra == \"tf_gpu\"->petastorm[opencv,tf_gpu]) (1.11.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu>=1.4.0; extra == \"tf_gpu\"->petastorm[opencv,tf_gpu]) (0.1.8)\n",
      "Requirement already satisfied: gast==0.2.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu>=1.4.0; extra == \"tf_gpu\"->petastorm[opencv,tf_gpu]) (0.2.2)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu>=1.4.0; extra == \"tf_gpu\"->petastorm[opencv,tf_gpu]) (0.33.6)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu>=1.4.0; extra == \"tf_gpu\"->petastorm[opencv,tf_gpu]) (1.1.0)\n",
      "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu>=1.4.0; extra == \"tf_gpu\"->petastorm[opencv,tf_gpu]) (2.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu>=1.4.0; extra == \"tf_gpu\"->petastorm[opencv,tf_gpu]) (2.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu>=1.4.0; extra == \"tf_gpu\"->petastorm[opencv,tf_gpu]) (3.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu>=1.4.0; extra == \"tf_gpu\"->petastorm[opencv,tf_gpu]) (1.26.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu>=1.4.0; extra == \"tf_gpu\"->petastorm[opencv,tf_gpu]) (0.8.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu>=1.4.0; extra == \"tf_gpu\"->petastorm[opencv,tf_gpu]) (0.9.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu>=1.4.0; extra == \"tf_gpu\"->petastorm[opencv,tf_gpu]) (3.9.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu>=1.4.0; extra == \"tf_gpu\"->petastorm[opencv,tf_gpu]) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu>=1.4.0; extra == \"tf_gpu\"->petastorm[opencv,tf_gpu]) (1.1.0)\n",
      "Requirement already satisfied: scikit-image>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (0.15.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu>=1.4.0; extra == \"tf_gpu\"->petastorm[opencv,tf_gpu]) (2.22.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu>=1.4.0; extra == \"tf_gpu\"->petastorm[opencv,tf_gpu]) (0.16.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu>=1.4.0; extra == \"tf_gpu\"->petastorm[opencv,tf_gpu]) (42.0.2.post20191201)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu>=1.4.0; extra == \"tf_gpu\"->petastorm[opencv,tf_gpu]) (3.1.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu>=1.4.0; extra == \"tf_gpu\"->petastorm[opencv,tf_gpu]) (0.4.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu>=1.4.0; extra == \"tf_gpu\"->petastorm[opencv,tf_gpu]) (1.10.0)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow-gpu>=1.4.0; extra == \"tf_gpu\"->petastorm[opencv,tf_gpu]) (2.9.0)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (3.1.1)\n",
      "Requirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4)\n",
      "Requirement already satisfied: pillow>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (6.2.1)\n",
      "Requirement already satisfied: imageio>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.6.1)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.1.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu>=1.4.0; extra == \"tf_gpu\"->petastorm[opencv,tf_gpu]) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu>=1.4.0; extra == \"tf_gpu\"->petastorm[opencv,tf_gpu]) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu>=1.4.0; extra == \"tf_gpu\"->petastorm[opencv,tf_gpu]) (1.25.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu>=1.4.0; extra == \"tf_gpu\"->petastorm[opencv,tf_gpu]) (2.8)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu>=1.4.0; extra == \"tf_gpu\"->petastorm[opencv,tf_gpu]) (1.3.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu>=1.4.0; extra == \"tf_gpu\"->petastorm[opencv,tf_gpu]) (0.2.8)\r\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu>=1.4.0; extra == \"tf_gpu\"->petastorm[opencv,tf_gpu]) (4.0)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu>=1.4.0; extra == \"tf_gpu\"->petastorm[opencv,tf_gpu]) (4.0.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (0.10.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.1.0)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (4.4.1)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu>=1.4.0; extra == \"tf_gpu\"->petastorm[opencv,tf_gpu]) (3.0.1)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu>=1.4.0; extra == \"tf_gpu\"->petastorm[opencv,tf_gpu]) (0.4.8)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install petastorm[opencv,tf_gpu] albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root = '/home/jovyan/work'\n",
    "\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "from petastorm.unischema import dict_to_spark_row, Unischema, UnischemaField\n",
    "from petastorm.codecs import ScalarCodec, CompressedImageCodec\n",
    "from petastorm.etl.dataset_metadata import get_schema_from_dataset_url, materialize_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Based on `core-site.xml` in https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/INSTALL.md doc\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"big_earth\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", root + \"/spark_dependencies/gcs-connector-hadoop2-latest.jar\") \\\n",
    "    .config(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\n",
    "    .config(\"fs.gs.project.id\", \"big-earth-252219\") \\\n",
    "    .config(\"google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .config(\"google.cloud.auth.service.account.json.keyfile\", root + \"/.gcs/big-earth-252219-fb2e5c109f78.json\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_url = 'file://' + root + '/data/hello_world_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# arrow_image_schema = pa.schema([\n",
    "#     ('image_name', pa.string()),\n",
    "#     ('image_bytes', pa.binary())\n",
    "# ])\n",
    "# arrow_image_struct = pa.struct([\n",
    "#     pa.field('image_name', pa.string()),\n",
    "#     pa.field('image_bytes', pa.binary())\n",
    "# ])\n",
    "\n",
    "# spark_image_schema = from_arrow_schema(arrow_image_schema)\n",
    "\n",
    "fields = [StructField(\"image_name\", StringType()), StructField(\"image_bytes\", BinaryType())]\n",
    "spark_image_schema = StructType(fields)\n",
    "\n",
    "\n",
    "def row_generator(x):\n",
    "    \"\"\"Returns a single entry in the generated dataset. Return a bunch of random values as an example.\"\"\"\n",
    "    arr =  np.random.randint(0, 4000, dtype=np.uint16, size=(120, 120, 3))\n",
    "#     p = pa.array([{'image_name': x, 'image_bytes': arr.tobytes()}], type=arrow_image_struct)\n",
    "    return [x, bytearray(arr)]\n",
    "\n",
    "\n",
    "def generate_hello_world_dataset(spark, output_url):\n",
    "    rows_count = 10000\n",
    "    filenames = [\"name_{}\".format(id) for id in range(rows_count)]\n",
    "    rowgroup_size_mb = 256\n",
    "\n",
    "    rows_rdd = sc.parallelize(filenames)\\\n",
    "       .map(row_generator)\n",
    "\n",
    "    spark.createDataFrame(rows_rdd, spark_image_schema) \\\n",
    "       .coalesce(10) \\\n",
    "       .write \\\n",
    "       .mode('overwrite') \\\n",
    "       .parquet(output_url)\n",
    "    \n",
    "    \n",
    "generate_hello_world_dataset(spark, output_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_build_nested_paths', '_get_column_indices', '_nested_paths_by_prefix', 'common_metadata', 'metadata', 'num_row_groups', 'read', 'read_row_group', 'read_row_groups', 'reader', 'scan_contents', 'schema']\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "parquet_files = [root + \"/data/hello_world_dataset/part-00000-a91053c4-cc36-41d2-bc08-578264106695-c000.snappy.parquet\"]\n",
    "parquet_file_row_groups = []\n",
    "total_num_rows = 0\n",
    "for parquet_file in parquet_files:\n",
    "    open_parquet_handle = pq.ParquetFile(parquet_file)\n",
    "    print(dir(open_parquet_handle))\n",
    "    for row_group in range(open_parquet_handle.num_row_groups):\n",
    "        parquet_file_row_groups.append(\n",
    "            (open_parquet_handle, row_group)\n",
    "        )\n",
    "#     total_num_rows += open_parquet_handle.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>image_bytes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>name_0</td>\n",
       "      <td>[[[3060, 3203, 2367], [640, 1081, 1414], [1299...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>name_1</td>\n",
       "      <td>[[[2134, 3184, 1565], [3816, 672, 3673], [2322...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>name_2</td>\n",
       "      <td>[[[3190, 747, 3926], [1821, 3602, 892], [28, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>name_3</td>\n",
       "      <td>[[[222, 3103, 1], [2272, 1603, 3605], [2253, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>name_4</td>\n",
       "      <td>[[[3392, 1654, 3643], [1955, 2045, 226], [1921...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547</th>\n",
       "      <td>name_1547</td>\n",
       "      <td>[[[2479, 2462, 905], [1010, 3360, 2925], [714,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1548</th>\n",
       "      <td>name_1548</td>\n",
       "      <td>[[[3867, 884, 959], [380, 918, 3609], [371, 32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549</th>\n",
       "      <td>name_1549</td>\n",
       "      <td>[[[399, 218, 304], [3253, 3508, 267], [3429, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>name_1550</td>\n",
       "      <td>[[[66, 2911, 3166], [3341, 1728, 1932], [404, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1551</th>\n",
       "      <td>name_1551</td>\n",
       "      <td>[[[268, 2818, 3927], [978, 3554, 2125], [2301,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1552 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_name                                        image_bytes\n",
       "0        name_0  [[[3060, 3203, 2367], [640, 1081, 1414], [1299...\n",
       "1        name_1  [[[2134, 3184, 1565], [3816, 672, 3673], [2322...\n",
       "2        name_2  [[[3190, 747, 3926], [1821, 3602, 892], [28, 1...\n",
       "3        name_3  [[[222, 3103, 1], [2272, 1603, 3605], [2253, 1...\n",
       "4        name_4  [[[3392, 1654, 3643], [1955, 2045, 226], [1921...\n",
       "...         ...                                                ...\n",
       "1547  name_1547  [[[2479, 2462, 905], [1010, 3360, 2925], [714,...\n",
       "1548  name_1548  [[[3867, 884, 959], [380, 918, 3609], [371, 32...\n",
       "1549  name_1549  [[[399, 218, 304], [3253, 3508, 267], [3429, 3...\n",
       "1550  name_1550  [[[66, 2911, 3166], [3341, 1728, 1932], [404, ...\n",
       "1551  name_1551  [[[268, 2818, 3927], [978, 3554, 2125], [2301,...\n",
       "\n",
       "[1552 rows x 2 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_file, row_group = parquet_file_row_groups[0]\n",
    "table = parquet_file.read_row_group(row_group)\n",
    "\n",
    "def deserialize_to_np(image_bytes):\n",
    "    return np.frombuffer(image_bytes, dtype=np.uint16).reshape(120, 120, 3)\n",
    "\n",
    "df = table.to_pandas()\n",
    "df['image_bytes'] = df['image_bytes'].apply(deserialize_to_np)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from decimal import Decimal\n",
    "\n",
    "\n",
    "_NUMPY_TO_TF_DTYPES_MAPPING = {\n",
    "    np.bool: tf.bool,\n",
    "    np.int8: tf.int8,\n",
    "    np.int16: tf.int16,\n",
    "    np.int32: tf.int32,\n",
    "    np.int64: tf.int64,\n",
    "    np.uint8: tf.uint8,\n",
    "    np.uint16: tf.int32,\n",
    "    np.uint32: tf.int64,\n",
    "    np.float32: tf.float32,\n",
    "    np.float64: tf.float64,\n",
    "    np.string_: tf.string,\n",
    "    np.unicode_: tf.string,\n",
    "    np.str_: tf.string,\n",
    "    np.bool_: tf.bool,\n",
    "    Decimal: tf.string,\n",
    "    np.datetime64: tf.int64,\n",
    "}\n",
    "\n",
    "def date_to_nsec_from_epoch(dt):\n",
    "    return timegm(dt.timetuple()) * 1000000000\n",
    "\n",
    "\n",
    "_date_to_nsec_from_epoch_vectorized = np.vectorize(date_to_nsec_from_epoch)\n",
    "\n",
    "\n",
    "def _sanitize_field_tf_types(sample):\n",
    "    \"\"\"Takes a named tuple and casts/promotes types unknown to TF to the types that are known.\n",
    "    Three casts that are currently implemented\n",
    "      - Decimal to string\n",
    "      - uint16 to int32\n",
    "      - np.datetime64 to int64, as nanoseconds since unix epoch\n",
    "    :param sample: named tuple or a dictionary\n",
    "    :return: same type as the input with values casted to types supported by Tensorflow\n",
    "    \"\"\"\n",
    "    next_sample_dict = sample._asdict()\n",
    "\n",
    "    for k, v in next_sample_dict.items():\n",
    "        if v is None:\n",
    "            raise RuntimeError('Encountered \"{}\"=None. Tensorflow does not support None values as a tensor.'\n",
    "                               'Consider filtering out these rows using a predicate.'.format(k))\n",
    "        # Assuming conversion to the same numpy type is trivial and dirty cheap\n",
    "        if isinstance(v, Decimal):\n",
    "            # Normalizing decimals only to get rid of the trailing zeros (makes testing easier, assuming has\n",
    "            # no other effect)\n",
    "            next_sample_dict[k] = str(v.normalize())\n",
    "        elif isinstance(v, np.ndarray) and np.issubdtype(v.dtype, np.datetime64):\n",
    "            # Convert to nanoseconds from POSIX epoch\n",
    "            next_sample_dict[k] = (v - np.datetime64('1970-01-01T00:00:00.0')) \\\n",
    "                .astype('timedelta64[ns]').astype(np.int64)\n",
    "        elif isinstance(v, np.ndarray) and v.dtype == np.uint16:\n",
    "            next_sample_dict[k] = v.astype(np.int32)\n",
    "        elif isinstance(v, np.ndarray) and v.dtype == np.uint32:\n",
    "            next_sample_dict[k] = v.astype(np.int64)\n",
    "        elif isinstance(v, np.ndarray) and v.dtype.type in (np.bytes_, np.unicode_):\n",
    "            if v.size != 0:\n",
    "                next_sample_dict[k] = v.tolist()\n",
    "        elif isinstance(v, np.ndarray) and v.dtype.kind == 'O' and isinstance(v[0], datetime.date):\n",
    "            # Pyarrow 0.12.1 started returning python datetime.date when parquet column is a DateType() column.\n",
    "            # Convert values in such column into nsec from epoch int64.\n",
    "            next_sample_dict[k] = _date_to_nsec_from_epoch_vectorized(v)\n",
    "\n",
    "    # Construct object of the same type as the input\n",
    "    return sample.__class__(**next_sample_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__next__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply_predicate_to_row_groups', '_apply_row_group_selector', '_create_ventilator', '_filter_row_groups', '_normalize_shuffle_options', '_partition_row_groups', '_results_queue_reader', '_workers_pool', 'batched_output', 'dataset', 'diagnostics', 'is_batched_reader', 'join', 'last_row_consumed', 'next', 'ngram', 'reset', 'schema', 'stop', 'stopped', 'ventilator']\n"
     ]
    }
   ],
   "source": [
    "from petastorm import make_batch_reader\n",
    "from petastorm.tf_utils import make_petastorm_dataset\n",
    "from albumentations import (\n",
    "    Compose, Flip, VerticalFlip, Resize, Rotate, ToFloat\n",
    ")\n",
    "\n",
    "augmentations_train = Compose([\n",
    "    Flip(p=0.5),\n",
    "    Rotate(limit=(0, 360), p=0.5)\n",
    "])\n",
    "\n",
    "\n",
    "def foo(img):\n",
    "    print(img)\n",
    "    return img\n",
    "    \n",
    "with make_batch_reader(output_url, num_epochs=2) as reader:\n",
    "    print(dir(reader))\n",
    "#     dataset = make_petastorm_dataset(reader).map(foo)\n",
    "#     for data in dataset:\n",
    "#         break\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

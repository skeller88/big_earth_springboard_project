{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_science.cnn_models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-228-2532557b97aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_science\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugmented_image_sequence_from_npy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAugmentedImageSequenceFromNpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_science\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_models\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbasic_cnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasic_cnn_model_with_best_practices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_science\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msklearn_batch_generator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSklearnBatchGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data_science.cnn_models'"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "import random\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "\n",
    "from albumentations import (\n",
    "    Compose, Flip, Rotate\n",
    ")\n",
    "\n",
    "import dask\n",
    "import dask.array as da\n",
    "from data_science.augmented_image_sequence_from_npy import AugmentedImageSequenceFromNpy\n",
    "from data_science.cnn_models import basic_cnn_model, basic_cnn_model_with_best_practices\n",
    "from data_science.sklearn_batch_generator import SklearnBatchGenerator\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support, precision_score, precision_recall_curve\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.metrics import Accuracy, Precision, Recall\n",
    "\n",
    "from tensorflow_addons.metrics import FBetaScore, F1Score\n",
    "\n",
    "\n",
    "pal = sns.color_palette()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/jovyan/work/data/big_earth'\n",
    "project_name = \"cloud_and_shadow\"\n",
    "log_dir = os.path.join(root, \"model/logs\")\n",
    "model_dir = os.path.join(root, \"model/models\")\n",
    "checkpoint_dir = os.path.join(root, \"model/checkpoints\")\n",
    "\n",
    "gcs_client = storage.Client()\n",
    "bucket = gcs_client.bucket(bucket_name)\n",
    "\n",
    "for directory in [log_dir, model_dir, checkpoint_dir]:\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 44)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# np.array(json.loads(df['binarized_labels'].iloc[0])).shape\n",
    "\n",
    "def prepare_data(df):\n",
    "    df['has_cloud_and_shadow_target'] = df['has_cloud_and_shadow_target'].apply(lambda x: np.array(json.loads(x)))\n",
    "    df['binarized_labels'] = df['binarized_labels'].apply(lambda x: np.array(json.loads(x)))    \n",
    "    df['image_path'] = root + \"/npy_image_files/\" + df['image_prefix'] + \".npy\"\n",
    "    return df\n",
    "\n",
    "df = pd.read_csv(root + \"/metadata/metadata.csv\")\n",
    "df = prepare_data(df)\n",
    "print(df['binarized_labels'].iloc[0].shape)\n",
    "print(df['has_cloud_and_shadow_target'].iloc[0].shape)\n",
    "df = df.set_index('image_prefix', drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# has_cloud_and_shadow = df[df['has_cloud_and_shadow'] == 1]\n",
    "# sample_no_cloud_and_shadow = df[df['has_cloud_and_shadow'] == 0].sample(\n",
    "#     n=len(has_cloud_and_shadow), random_state=random_seed)\n",
    "\n",
    "# print(\"len(sample_no_cloud_and_shadow)\", len(sample_no_cloud_and_shadow), \"len(has_cloud_and_shadow)\", \n",
    "#       len(has_cloud_and_shadow))\n",
    "\n",
    "# train, valid, test = balanced_class_train_test_splits(*[sample_no_cloud_and_shadow, has_cloud_and_shadow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1907 253 240\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "google_automl_dataset = pd.read_csv(root + '/google_automl_cloud_and_shadow_dataset_small.csv')\n",
    "google_automl_dataset['image_prefix'] = google_automl_dataset['gcs_uri'].str.split('/').apply(lambda x: x[-1].replace(\".png\", \"\"))\n",
    "google_automl_dataset = google_automl_dataset.set_index('image_prefix', drop=False)\n",
    "\n",
    "train = df.loc[google_automl_dataset[google_automl_dataset['set'] == 'TRAIN'].index]\n",
    "valid = df.loc[google_automl_dataset[google_automl_dataset['set'] == 'VALIDATION'].index]\n",
    "test = df.loc[google_automl_dataset[google_automl_dataset['set'] == 'TEST'].index]\n",
    "\n",
    "print(len(train), len(valid), len(test))\n",
    "print(len(train) + len(valid) + len(test) == len(google_automl_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.17684245109558\n"
     ]
    }
   ],
   "source": [
    "# npy_image_dir = root + \"/npy_image_files\"\n",
    "# npy_files = [npy_image_dir + \"/\" + file + \".npy\" for file in train['image_prefix'].values]\n",
    "# start = time.time()\n",
    "# stats = stats_for_numpy_images(npy_files,  use_test_data=False)\n",
    "# stats.to_csv('cloud_and_shadow_stats.csv', index_label='band')\n",
    "# print(time.time() - start)\n",
    "\n",
    "stats = pd.read_csv('cloud_and_shadow_stats.csv', index_col='band')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x_train: np.array, y_train: np.array, x_valid: np.array,\n",
    "          y_valid: np.array, n_epochs, n_classes, batch_size, log_dir, model_path):\n",
    "    \"\"\"\n",
    "    Based on from https://www.kaggle.com/infinitewing/keras-solution-and-my-experience-0-92664\n",
    "    \"\"\"\n",
    "    print(f'Split train: {len(x_train)}')\n",
    "    print(f'Split valid: {len(x_valid)}')\n",
    "\n",
    "    histories = []\n",
    "    learn_rates = [0.001, 0.0001, 0.00001]\n",
    "    metrics = [Accuracy(), Precision(), Recall(), F1Score(num_classes=n_classes, average='micro'),\n",
    "               FBetaScore(num_classes=n_classes, beta=2.0, average='micro')]\n",
    "    loss = 'binary_crossentropy'\n",
    "    metric_to_monitor = 'val_loss'\n",
    "\n",
    "    for learn_rate_num, learn_rate in enumerate(learn_rates):\n",
    "        print(f'Training model on fold with learn_rate {learn_rate}')\n",
    "        optimizer = Adam(lr=learn_rate, momentum=0.9)\n",
    "        model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "        verbosity = 0\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor=metric_to_monitor, patience=2, verbose=verbosity),\n",
    "            ReduceLROnPlateau(monitor=metric_to_monitor, factor=0.5, patience=2, min_lr=0.000001),\n",
    "            TensorBoard(log_dir, histogram_freq=1),\n",
    "            ModelCheckpoint(model_path, monitor=metric_to_monitor, save_weights_only=False, save_best_only=True,\n",
    "                            verbose=verbosity)\n",
    "        ]\n",
    "\n",
    "        # Generators\n",
    "        train_generator = AugmentedImageSequenceFromNpy(x=x_train, y=y_train, batch_size=batch_size,\n",
    "                                                        augmentations=AUGMENTATIONS_TRAIN)\n",
    "\n",
    "        valid_generator = AugmentedImageSequenceFromNpy(x=x_valid, y=y_valid, batch_size=batch_size,\n",
    "                                                        augmentations=AUGMENTATIONS_TEST)\n",
    "\n",
    "        history = model.fit_generator(generator=train_generator,\n",
    "                                      epochs=n_epochs,\n",
    "                                      steps_per_epoch=len(train_generator),\n",
    "                                      callbacks=callbacks,\n",
    "                                      validation_data=valid_generator, validation_steps=len(valid_generator),\n",
    "                                      shuffle=True, verbose=1)\n",
    "        histories.append(history)\n",
    "\n",
    "    # Attempt to avoid memory leaks\n",
    "    del train_generator\n",
    "    del valid_generator\n",
    "    gc.collect()\n",
    "\n",
    "    return histories\n",
    "\n",
    "\n",
    "def join_histories(histories):\n",
    "    full_history = defaultdict(list)\n",
    "\n",
    "    for history in histories:\n",
    "        for key, value in history.history.items():\n",
    "            full_history[key].extend(value)\n",
    "    return full_history\n",
    "\n",
    "\n",
    "def graph_model_history(history):\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    t = f.suptitle('Basic CNN Performance', fontsize=12)\n",
    "    f.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "    max_epoch = len(history['val_loss']) + 1\n",
    "    epoch_list = list(range(1, max_epoch))\n",
    "    ax1.plot(epoch_list, history['accuracy'], label='Train Accuracy')\n",
    "    ax1.plot(epoch_list, history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax1.set_xticks(np.arange(1, max_epoch, 5))\n",
    "    ax1.set_ylabel('Accuracy Value')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_title('Accuracy')\n",
    "    l1 = ax1.legend(loc=\"best\")\n",
    "\n",
    "    ax2.plot(epoch_list, history['loss'], label='Train Loss')\n",
    "    ax2.plot(epoch_list, history['val_loss'], label='Validation Loss')\n",
    "    ax2.set_xticks(np.arange(1, max_epoch, 5))\n",
    "    ax2.set_ylabel('Loss Value')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_title('Loss')\n",
    "    l2 = ax2.legend(loc=\"best\")\n",
    "\n",
    "\n",
    "def predict(model, model_path, x, batch_size, n_classes):\n",
    "    thresholds = np.array([0.5 for _ in range(n_classes)])\n",
    "    model.load_weights(model_path)\n",
    "\n",
    "    predict_generator = AugmentedImageSequenceFromNpy(x=x, y=None, batch_size=batch_size,\n",
    "                                                        augmentations=AUGMENTATIONS_TEST)\n",
    "    # Generators\n",
    "    pred_test_probs = model.predict_generator(predict_generator)\n",
    "    pred_test_labels = pd.DataFrame(pred_test_probs, columns=classes)\n",
    "    pred_test_labels = pred_test_labels.apply(lambda x: x > thresholds, axis=1)\n",
    "    # Convert boolean predictions to labels\n",
    "    pred_test_lables = pred_test_labels.apply(lambda row: ' '.join(row[row].index), axis=1)\n",
    "\n",
    "    del predict_generator\n",
    "    gc.collect()\n",
    "\n",
    "    return pred_test_labels\n",
    "\n",
    "AUGMENTATIONS_TRAIN = Compose([\n",
    "    Flip(p=0.5),\n",
    "    Rotate(limit=(0, 360), p=0.5)\n",
    "])\n",
    "\n",
    "AUGMENTATIONS_TEST = Compose([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1907, 1) (1,)\n",
      "(128, 120, 120, 3) (128, 1)\n"
     ]
    }
   ],
   "source": [
    "n_classes = 1\n",
    "\n",
    "n_epochs = 100\n",
    "model = basic_cnn_model((120, 120, 3), n_classes=n_classes)\n",
    "\n",
    "# Test the correctness and speed of loading one batch\n",
    "batch_size = 128\n",
    "\n",
    "x_train = train['image_path'].values\n",
    "x_valid = valid['image_path'].values\n",
    "x_test = test['image_path'].values\n",
    "\n",
    "target = 'has_cloud_and_shadow_target'\n",
    "y_train = np.stack(train[target].values)\n",
    "y_valid = np.stack(valid[target].values)\n",
    "y_test = np.stack(test[target].values)\n",
    "\n",
    "print(y_train.shape, y_train[0].shape)\n",
    "\n",
    "use_small_dataset = True\n",
    "use_random_small_dataset = False\n",
    "if use_small_dataset:\n",
    "    x_train = np.concatenate([x_train[:50], x_train[50:]])\n",
    "    x_valid = np.concatenate([x_valid[:50], x_valid[50:]])\n",
    "    x_test = np.concatenate([x_test[:50], x_test[50:]])\n",
    "\n",
    "    y_train = np.concatenate([y_train[:50], y_train[50:]])\n",
    "    y_valid = np.concatenate([y_valid[:50], y_valid[50:]])\n",
    "    y_test = np.concatenate([y_test[:50], y_test[50:]])\n",
    "elif use_random_small_dataset:\n",
    "    shape = (100, 1)\n",
    "    x_train = np.concatenate([np.ones(shape), np.zeros(shape)])\n",
    "    y_train = np.concatenate([np.ones(shape), np.zeros(shape)])\n",
    "\n",
    "    x_valid = np.concatenate([np.ones(shape), np.zeros(shape)])\n",
    "    y_valid = np.concatenate([np.ones(shape), np.zeros(shape)])\n",
    "\n",
    "    y_train = np.random.randint(0, 2, (len(train), 44))\n",
    "    y_valid = np.random.randint(0, 2, (len(valid), 44))\n",
    "    y_test = np.random.randint(0, 2, (len(test), 44))\n",
    "    y_test_labels = test['labels'].values\n",
    "\n",
    "a = AugmentedImageSequenceFromNpy(x=x_train, y=y_train,\n",
    "                                  batch_size=batch_size,\n",
    "                                  augmentations=AUGMENTATIONS_TRAIN, stats=stats)\n",
    "\n",
    "for x, y in a:\n",
    "    print(x.shape, y.shape)\n",
    "    break\n",
    "\n",
    "a.on_epoch_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 43200) (43200,) (128,) ()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5625"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# sanity check the generator output\n",
    "train_batch_generator = SklearnBatchGenerator(x_train, y_train, batch_size, AUGMENTATIONS_TRAIN, stats)\n",
    "valid_batch_generator = SklearnBatchGenerator(x_valid, y_valid, batch_size, AUGMENTATIONS_TEST, stats)\n",
    "\n",
    "train_batch_generator.on_epoch_end()\n",
    "valid_batch_generator.on_epoch_end()\n",
    "\n",
    "clf = LogisticRegression()\n",
    "x, y = train_batch_generator[0]\n",
    "print(x.shape, x[0].shape, y.shape, y[0].shape)\n",
    "clf.fit(x, y)\n",
    "\n",
    "x, y = valid_batch_generator[0]\n",
    "pred = clf.predict(x)\n",
    "accuracy_score(pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training completed in 198.89089703559875 seconds\n",
      "epoch_num 0 - 395.3008 sec - 0.6521739130434783\n",
      "epoch_num 1 - 373.6191 sec - 0.43873517786561267\n",
      "epoch_num 2 - 357.0683 sec - 0.4782608695652174\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'work_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-226-fcbf2350238e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mepochs_without_improvement\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwork_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{model_name}.joblib\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_metadata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             json.dump({\n",
      "\u001b[0;31mNameError\u001b[0m: name 'work_dir' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "clf = SGDClassifier()\n",
    "history = list()\n",
    "\n",
    "train_batch_generator = SklearnBatchGenerator(x_train, y_train, batch_size, AUGMENTATIONS_TRAIN, stats)\n",
    "valid_batch_generator = SklearnBatchGenerator(x_valid, y_valid, batch_size, AUGMENTATIONS_TEST, stats)\n",
    "\n",
    "n_epochs = 100\n",
    "n_batches = len(x_train) // batch_size\n",
    "classes = np.array([0, 1])\n",
    "early_stopping_patience = 6\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "now = datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M\")\n",
    "experiment_name = f\"sgd_classifier_default_{now}\"\n",
    "model_path = os.path.join(model_dir, experiment_name + \".joblib\")\n",
    "model_metadata_path = os.path.join(model_dir, experiment_name + \"_metadata.json\")\n",
    "\n",
    "# Shuffle the data\n",
    "train_batch_generator.on_epoch_end()\n",
    "valid_batch_generator.on_epoch_end()\n",
    "for epoch in range(n_epochs):\n",
    "    start = time.time()\n",
    "    for batch_x, batch_y in train_batch_generator.make_one_shot_iterator():\n",
    "        clf.partial_fit(batch_x, batch_y, classes=classes)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"training completed in\", time.time() - start, \"seconds\")\n",
    "        \n",
    "    actual_y_train, pred_y_train = train_batch_generator.get_predictions(clf)\n",
    "    actual_y_valid, pred_y_valid = valid_batch_generator.get_predictions(clf)\n",
    "\n",
    "    epoch_time = f\"{time.time() - start:.4f}\"\n",
    "    epoch_metrics = {\n",
    "        'accuracy_train': accuracy_score(actual_y_train, pred_y_train),\n",
    "        'accuracy_valid': accuracy_score(actual_y_valid, pred_y_valid),        \n",
    "        \"f1_score_train\": f1_score(actual_y_train, pred_y_train),\n",
    "        \"f1_score_valid\": f1_score(actual_y_valid, pred_y_valid),        \n",
    "    }\n",
    "    history.append(epoch_metrics)\n",
    "    \n",
    "    print(\"epoch_num\", epoch, \"-\", epoch_time, \"sec -\", epoch_metrics['accuracy_valid'])\n",
    "        \n",
    "    if len(history) < 2:\n",
    "        continue\n",
    "        \n",
    "    if epoch_metrics['accuracy_valid'] <= history[-2]['accuracy_valid']:\n",
    "        epochs_without_improvement += 1\n",
    "    else:\n",
    "        dump(clf, model_path)\n",
    "        with open(model_metadata_path, 'w+') as json_file:\n",
    "            json.dump({\n",
    "                'data': 'npy_image_files_cloud_and_shadow_equal_split_no_cloud_and_shadow',\n",
    "                'data_prep': 'normalization_augmentation',\n",
    "                'experiment_name': experiment_name,\n",
    "                'experiment_start_time': now,\n",
    "                'model': SGDClassifier.__name__,\n",
    "                'random_state': random_seed,\n",
    "                'confusion_matrix': confusion_matrix(actual_y_valid, pred_y_valid),\n",
    "                'precision_recall_curve': precision_recall_curve(actual_y_valid, pred_y_valid),\n",
    "                'history': history\n",
    "            }, json_file)\n",
    "        \n",
    "        for filename in [model_path, model_metadata_path]:\n",
    "            blob = bucket.blob(filename)\n",
    "            blob.upload_from_filename(filename)\n",
    "            \n",
    "        epochs_without_improvement = 0\n",
    "    \n",
    "    if epochs_without_improvement == early_stopping_patience:\n",
    "        print(\"Ending training due to no improvement\")\n",
    "        break\n",
    "                                    \n",
    "    train_batch_generator.on_epoch_end()\n",
    "    valid_batch_generator.on_epoch_end()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if os.environ.get(\"SHOULD_TRAIN\", \"True\") == \"True\":\n",
    "    now = datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M\")\n",
    "    experiment_name = f\"{project_name}_basic_cnn_{now}\"\n",
    "    model_path = os.path.join(model_dir, experiment_name)\n",
    "    cnn_checkpoint_path = os.path.join(checkpoint_dir, f\"{experiment_name}.cpkt\")\n",
    "\n",
    "#     histories = train(model, x_train=x_train,\n",
    "#                       y_train=y_train,\n",
    "#                       x_valid=x_valid,\n",
    "#                       y_valid=y_valid,\n",
    "#                       n_epochs=n_epochs,\n",
    "#                       n_classes=n_classes,\n",
    "#                       batch_size=batch_size,\n",
    "#                       log_dir=log_dir,\n",
    "#                       model_path=model_path)\n",
    "    \n",
    "    print(f'len(train): {len(x_train)}')\n",
    "    print(f'len(valid): {len(x_valid)}')\n",
    "\n",
    "    histories = []\n",
    "    f1_score = F1Score(num_classes=n_classes)\n",
    "    metrics = [Accuracy(), Precision(), Recall(), f1_score,\n",
    "               FBetaScore(num_classes=n_classes, beta=2.0)]\n",
    "    loss = 'binary_crossentropy'\n",
    "    metric_to_monitor = 'val_accuracy'\n",
    "\n",
    "    optimizer = Adam()\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "    verbosity = 0\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=metric_to_monitor, patience=early_stopping_patience, verbose=verbosity),\n",
    "        ReduceLROnPlateau(monitor=metric_to_monitor, factor=0.5, patience=early_stopping_patience, min_lr=0.000001),\n",
    "        TensorBoard(log_dir, histogram_freq=1),\n",
    "        ModelCheckpoint(cnn_checkpoint_path, monitor=metric_to_monitor, save_weights_only=False, save_best_only=True,\n",
    "                        verbose=verbosity)\n",
    "    ]\n",
    "\n",
    "    # Generators\n",
    "    train_generator = AugmentedImageSequenceFromNpy(x=x_train, y=y_train, batch_size=batch_size,\n",
    "                                                    augmentations=AUGMENTATIONS_TRAIN)\n",
    "\n",
    "    valid_generator = AugmentedImageSequenceFromNpy(x=x_valid, y=y_valid, batch_size=batch_size,\n",
    "                                                    augmentations=AUGMENTATIONS_TEST)\n",
    "\n",
    "    history = model.fit_generator(train_generator,\n",
    "                                  epochs=n_epochs,\n",
    "                                  steps_per_epoch=len(train_generator),\n",
    "                                  callbacks=callbacks,\n",
    "                                  validation_data=valid_generator, validation_steps=len(valid_generator),\n",
    "                                  shuffle=True, verbose=1)\n",
    "    \n",
    "    model.save(model_path)\n",
    "    # Attempt to avoid memory leaks\n",
    "    del train_generator\n",
    "    del valid_generator\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "\n",
    "# if os.environ.get(\"SHOULD_PREDICT\", \"True\") == \"True\":\n",
    "#     pred_test_labels = predict(model=model, weight_dir=model_path, x=x_test, batch_size=batch_size, n_classes=n_classes)\n",
    "#     clf_report = classification_report(y_test_labels, pred_test_labels, target_names=classes)\n",
    "#     print(clf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_generator = AugmentedImageSequenceFromNpy(x=x_test, y=None, batch_size=batch_size,\n",
    "                                                        augmentations=AUGMENTATIONS_TEST)\n",
    "y_pred = model.predict(test_generator)\n",
    "y_pred_binary = [0 if pred < .5 else 1 for pred in y_pred]\n",
    "clf = classification_report(y_test, y_pred_binary,  target_names=['has_clouds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(y_pred)[0].unique())\n",
    "print(pd.DataFrame(y_pred_binary)[0].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_pred)[0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_pred_binary)[0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_histories = join_histories(histories)\n",
    "graph_model_history(full_histories)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "from joblib import dump, load\n",
    "import random\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "\n",
    "from albumentations import (\n",
    "    Compose, Flip, Rotate\n",
    ")\n",
    "\n",
    "import dask\n",
    "import dask.array as da\n",
    "from google.cloud import storage\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support, precision_score, precision_recall_curve\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "\n",
    "from data_engineering.dask_image_stats_collector import stats_for_numpy_images \n",
    "from data_science.augmented_image_sequence_from_npy import AugmentedImageSequenceFromNpy\n",
    "from data_science.keras.model_checkpoint_gcs import ModelCheckpointGCS\n",
    "from data_science.keras.cnn_models import basic_cnn_model, basic_cnn_model_with_best_practices\n",
    "from data_science.serialization_utils import numpy_to_json, sklearn_precision_recall_curve_to_dict\n",
    "from data_science.sklearn_batch_generator import SklearnBatchGenerator\n",
    "from data_science.train import get_model_and_metadata_from_gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "n_classes = 1\n",
    "n_epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "early_stopping_patience = 6\n",
    "\n",
    "use_small_dataset = True\n",
    "use_random_small_dataset = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/app/.gcs/big-earth-252219-fb2e5c109f78.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pal = sns.color_palette()\n",
    "\n",
    "random_seed = 0\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "root = '/home/jovyan/work/data/big_earth'\n",
    "project_name = \"cloud_and_shadow\"\n",
    "model_dir = os.path.join(root, \"model/models\")\n",
    "# blob prefix\n",
    "gcs_model_dir = \"model/models\"\n",
    "# tensorboard\n",
    "gcs_log_dir = \"gs://big_earth/model/logs\"\n",
    "\n",
    "for directory in [model_dir]:\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logger = logging.getLogger('papermill')\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "logger.info('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_client = storage.Client()\n",
    "bucket = gcs_client.bucket(\"big_earth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 44)\n",
      "(1, 44)\n",
      "(1,)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# np.array(json.loads(df['binarized_labels'].iloc[0])).shape\n",
    "\n",
    "def prepare_data(df):\n",
    "    df['has_cloud_and_shadow_target'] = df['has_cloud_and_shadow_target'].apply(lambda x: np.array(json.loads(x)))\n",
    "    df['binarized_labels'] = df['binarized_labels'].apply(lambda x: np.array(json.loads(x)))    \n",
    "    df['image_path'] = root + \"/npy_image_files/\" + df['image_prefix'] + \".npy\"\n",
    "    return df\n",
    "\n",
    "df = pd.read_csv(root + \"/metadata/metadata.csv\")\n",
    "df = prepare_data(df)\n",
    "logger.info(df['binarized_labels'].iloc[0].shape)\n",
    "logger.info(df['has_cloud_and_shadow_target'].iloc[0].shape)\n",
    "df = df.set_index('image_prefix', drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1907 253 240\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# len(pd.read_csv(root + '/google_automl_cloud_and_shadow_dataset.csv'))\n",
    "\n",
    "google_automl_dataset = pd.read_csv( '/app/data_science/google_automl_cloud_and_shadow_dataset_small.csv')\n",
    "google_automl_dataset['image_prefix'] = google_automl_dataset['gcs_uri'].str.split('/').apply(lambda x: x[-1].replace(\".png\", \"\"))\n",
    "google_automl_dataset = google_automl_dataset.set_index('image_prefix', drop=False)\n",
    "\n",
    "train = df.loc[google_automl_dataset[google_automl_dataset['set'] == 'TRAIN'].index]\n",
    "valid = df.loc[google_automl_dataset[google_automl_dataset['set'] == 'VALIDATION'].index]\n",
    "test = df.loc[google_automl_dataset[google_automl_dataset['set'] == 'TEST'].index]\n",
    "\n",
    "print(len(train), len(valid), len(test))\n",
    "print(len(train) + len(valid) + len(test) == len(google_automl_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(root + '/cloud_and_shadow_stats.csv'):\n",
    "    stats = pd.read_csv(root + '/cloud_and_shadow_stats.csv', index_col='band')\n",
    "else:\n",
    "    npy_image_dir = root + \"/npy_image_files\"\n",
    "    npy_files = [npy_image_dir + \"/\" + file + \".npy\" for file in train['image_prefix'].values]\n",
    "    start = time.time()\n",
    "    stats = stats_for_numpy_images(npy_files,  use_test_data=False)\n",
    "    stats.to_csv(root + '/cloud_and_shadow_stats.csv', index_label='band')\n",
    "    logger.info(f'stats computed in {time.time() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_histories(histories):\n",
    "    full_history = defaultdict(list)\n",
    "\n",
    "    for history in histories:\n",
    "        for key, value in history.history.items():\n",
    "            full_history[key].extend(value)\n",
    "    return full_history\n",
    "\n",
    "\n",
    "def graph_model_history(history):\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    t = f.suptitle('Basic CNN Performance', fontsize=12)\n",
    "    f.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "    max_epoch = len(history['val_loss']) + 1\n",
    "    epoch_list = list(range(1, max_epoch))\n",
    "    ax1.plot(epoch_list, history['accuracy'], label='Train Accuracy')\n",
    "    ax1.plot(epoch_list, history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax1.set_xticks(np.arange(1, max_epoch, 5))\n",
    "    ax1.set_ylabel('Accuracy Value')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_title('Accuracy')\n",
    "    l1 = ax1.legend(loc=\"best\")\n",
    "\n",
    "    ax2.plot(epoch_list, history['loss'], label='Train Loss')\n",
    "    ax2.plot(epoch_list, history['val_loss'], label='Validation Loss')\n",
    "    ax2.set_xticks(np.arange(1, max_epoch, 5))\n",
    "    ax2.set_ylabel('Loss Value')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_title('Loss')\n",
    "    l2 = ax2.legend(loc=\"best\")\n",
    "\n",
    "\n",
    "def predict(model, model_path, x, batch_size, n_classes):\n",
    "    thresholds = np.array([0.5 for _ in range(n_classes)])\n",
    "    model.load_weights(model_path)\n",
    "\n",
    "    predict_generator = AugmentedImageSequenceFromNpy(x=x, y=None, batch_size=batch_size,\n",
    "                                                        augmentations=AUGMENTATIONS_TEST)\n",
    "    # Generators\n",
    "    pred_test_probs = model.predict_generator(predict_generator)\n",
    "    pred_test_labels = pd.DataFrame(pred_test_probs, columns=classes)\n",
    "    pred_test_labels = pred_test_labels.apply(lambda x: x > thresholds, axis=1)\n",
    "    # Convert boolean predictions to labels\n",
    "    pred_test_lables = pred_test_labels.apply(lambda row: ' '.join(row[row].index), axis=1)\n",
    "\n",
    "    del predict_generator\n",
    "    gc.collect()\n",
    "\n",
    "    return pred_test_labels\n",
    "\n",
    "AUGMENTATIONS_TRAIN = Compose([\n",
    "    Flip(p=0.5),\n",
    "    Rotate(limit=(0, 360), p=0.5)\n",
    "])\n",
    "\n",
    "AUGMENTATIONS_TEST = Compose([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1907, 1) (1,)\n",
      "(100, 120, 120, 3) (100, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train = train['image_path'].values\n",
    "x_valid = valid['image_path'].values\n",
    "x_test = test['image_path'].values\n",
    "\n",
    "target = 'has_cloud_and_shadow_target'\n",
    "y_train = np.stack(train[target].values)\n",
    "y_valid = np.stack(valid[target].values)\n",
    "y_test = np.stack(test[target].values)\n",
    "\n",
    "print(y_train.shape, y_train[0].shape)\n",
    "\n",
    "if use_small_dataset:\n",
    "    n_epochs = 3\n",
    "    x_train = np.concatenate([x_train[:50], x_train[-50:]])\n",
    "    x_valid = np.concatenate([x_valid[:50], x_valid[-50:]])\n",
    "    x_test = np.concatenate([x_test[:50], x_test[-50:]])\n",
    "\n",
    "    y_train = np.concatenate([y_train[:50], y_train[-50:]])\n",
    "    y_valid = np.concatenate([y_valid[:50], y_valid[-50:]])\n",
    "    y_test = np.concatenate([y_test[:50], y_test[-50:]])\n",
    "elif use_random_small_dataset:\n",
    "    shape = (100, 1)\n",
    "    x_train = np.concatenate([np.ones(shape), np.zeros(shape)])\n",
    "    y_train = np.concatenate([np.ones(shape), np.zeros(shape)])\n",
    "\n",
    "    x_valid = np.concatenate([np.ones(shape), np.zeros(shape)])\n",
    "    y_valid = np.concatenate([np.ones(shape), np.zeros(shape)])\n",
    "\n",
    "    y_train = np.random.randint(0, 2, (len(train), 44))\n",
    "    y_valid = np.random.randint(0, 2, (len(valid), 44))\n",
    "    y_test = np.random.randint(0, 2, (len(test), 44))\n",
    "    y_test_labels = test['labels'].values\n",
    "\n",
    "a = AugmentedImageSequenceFromNpy(x=x_train, y=y_train,\n",
    "                                  batch_size=batch_size,\n",
    "                                  augmentations=AUGMENTATIONS_TRAIN, stats=stats)\n",
    "\n",
    "for x, y in a:\n",
    "    print(x.shape, y.shape)\n",
    "    break\n",
    "\n",
    "a.on_epoch_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 43200) (43200,) (100,) ()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.58"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# sanity check the generator output\n",
    "train_batch_generator = SklearnBatchGenerator(x_train, y_train, batch_size, AUGMENTATIONS_TRAIN, stats)\n",
    "valid_batch_generator = SklearnBatchGenerator(x_valid, y_valid, batch_size, AUGMENTATIONS_TEST, stats)\n",
    "\n",
    "train_batch_generator.on_epoch_end()\n",
    "valid_batch_generator.on_epoch_end()\n",
    "\n",
    "clf = LogisticRegression()\n",
    "x, y = train_batch_generator[0]\n",
    "print(x.shape, x[0].shape, y.shape, y[0].shape)\n",
    "clf.fit(x, y)\n",
    "\n",
    "x, y = valid_batch_generator[0]\n",
    "pred = clf.predict(x)\n",
    "accuracy_score(pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_num 1 - 2.3652 sec - 0.58\n",
      "epoch_num 2 - 2.0518 sec - 0.58\n"
     ]
    }
   ],
   "source": [
    "import joblib \n",
    "\n",
    "history = list()\n",
    "\n",
    "train_batch_generator = SklearnBatchGenerator(x_train, y_train, batch_size, AUGMENTATIONS_TRAIN, stats)\n",
    "valid_batch_generator = SklearnBatchGenerator(x_valid, y_valid, batch_size, AUGMENTATIONS_TEST, stats)\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "now = datetime.datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "experiment_name = f\"sgd_classifier_default_2020_1_31\"\n",
    "gcs_model_dir = \"model/models\"\n",
    "model_path = os.path.join(model_dir, experiment_name + \".joblib\")\n",
    "model_gcs_path = os.path.join(gcs_model_dir, experiment_name + \".joblib\")\n",
    "model_metadata_path = os.path.join(model_dir, experiment_name + \"_metadata.json\")\n",
    "model_metadata_gcs_path = os.path.join(gcs_model_dir, experiment_name + \"_metadata.json\")\n",
    "\n",
    "model, model_base_metadata = get_model_and_metadata_from_gcs(bucket, model_dir, \"joblib\", joblib.load, gcs_model_dir, \n",
    "                                                             experiment_name)\n",
    "\n",
    "if model is not None:\n",
    "    print('Resuming training at epoch', model_base_metadata['epoch'])\n",
    "else:\n",
    "    model = SGDClassifier()\n",
    "    model_base_metadata = {\n",
    "        'data': 'train_valid_google_automl_cloud_and_shadow_dataset_small.csv',\n",
    "        'data_prep': 'normalization_augmentation',\n",
    "        'experiment_name': experiment_name,\n",
    "        'experiment_start_time': now,\n",
    "        'model': SGDClassifier.__name__,\n",
    "        'random_state': random_seed,\n",
    "        'epoch': 0\n",
    "    }\n",
    "        \n",
    "# Shuffle the data\n",
    "train_batch_generator.on_epoch_end()\n",
    "valid_batch_generator.on_epoch_end()\n",
    "train_start = time.time()\n",
    "best_model = None\n",
    "for epoch in range(int(model_base_metadata['epoch']) + 1, n_epochs):\n",
    "    start = time.time()\n",
    "    for batch_x, batch_y in train_batch_generator.make_one_shot_iterator():\n",
    "        model.partial_fit(batch_x, batch_y, classes=classes)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"training completed in\", time.time() - start, \"seconds\")\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    actual_y_train, pred_y_train = train_batch_generator.get_predictions(clf)\n",
    "    actual_y_valid, pred_y_valid = valid_batch_generator.get_predictions(clf)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"prediction completed in\", time.time() - start, \"seconds\")\n",
    "\n",
    "    epoch_time = f\"{time.time() - start:.4f}\"\n",
    "    epoch_metrics = {\n",
    "        'accuracy_train': sklearn.metrics.accuracy_score(actual_y_train, pred_y_train),\n",
    "        'accuracy_valid': sklearn.metrics.accuracy_score(actual_y_valid, pred_y_valid),        \n",
    "        \"f1_score_train\": sklearn.metrics.f1_score(actual_y_train, pred_y_train),\n",
    "        \"f1_score_valid\": sklearn.metrics.f1_score(actual_y_valid, pred_y_valid),        \n",
    "    }\n",
    "    history.append(epoch_metrics)\n",
    "    \n",
    "    print(\"epoch_num\", epoch, \"-\", epoch_time, \"sec -\", epoch_metrics['accuracy_valid'])\n",
    "        \n",
    "    if len(history) < 2:\n",
    "        continue\n",
    "        \n",
    "    if epoch_metrics['accuracy_valid'] <= history[-2]['accuracy_valid']:\n",
    "        epochs_without_improvement += 1\n",
    "    else:\n",
    "        dump(model, model_path)\n",
    "        with open(model_metadata_path, 'w+') as json_file:\n",
    "            model_base_metadata.update({\n",
    "                'epoch': str(epoch),\n",
    "                'confusion_matrix': numpy_to_json(confusion_matrix(actual_y_valid, pred_y_valid)),\n",
    "                'precision_recall_curve': sklearn_precision_recall_curve_to_dict(\n",
    "                    sklearn.metrics.precision_recall_curve(actual_y_valid, pred_y_valid)),\n",
    "                'history': history,\n",
    "                'train_time_elapsed': time.time() - train_start\n",
    "            })\n",
    "            json.dump(model_base_metadata, json_file)\n",
    "        \n",
    "        for filename, gcs_filename in [(model_path, model_gcs_path), (model_metadata_path, model_metadata_gcs_path)]:\n",
    "            blob = bucket.blob(gcs_filename)\n",
    "            blob.upload_from_filename(filename)\n",
    "            \n",
    "        epochs_without_improvement = 0\n",
    "    \n",
    "    if epochs_without_improvement == early_stopping_patience:\n",
    "        print(\"Ending training due to no improvement\")\n",
    "        break\n",
    "    \n",
    "    \n",
    "    train_batch_generator.on_epoch_end()\n",
    "    valid_batch_generator.on_epoch_end()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_keras_model(*, bucket, model_dir, gcs_model_dir, gcs_log_dir, experiment_name, start_model):\n",
    "    model, model_base_metadata = get_model_and_metadata_from_gcs(bucket, model_dir, \"h5\", load_model, gcs_model_dir, experiment_name)\n",
    "\n",
    "    model_and_metadata_filepath = os.path.join(model_dir, experiment_name)\n",
    "    gcs_model_and_metadata_filepath = os.path.join(gcs_model_dir, experiment_name)\n",
    "    gcs_log_dir = os.path.join(gcs_log_dir, experiment_name)\n",
    "\n",
    "    if model is not None:\n",
    "        print('Resuming training at epoch', int(model_base_metadata['epoch']) + 1)\n",
    "    else:\n",
    "        now = datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M\")\n",
    "        model = start_model\n",
    "        model_base_metadata = {\n",
    "            'data': 'train_valid_google_automl_cloud_and_shadow_dataset_small.csv',\n",
    "            'data_prep': 'normalization_augmentation',\n",
    "            'experiment_name': experiment_name,\n",
    "            'experiment_start_time': now,\n",
    "            'model': 'keras_cnn',\n",
    "            'random_state': random_seed,\n",
    "            # so that initial_epoch is 0\n",
    "            'epoch': -1\n",
    "        }        \n",
    "\n",
    "    print(f'len(train): {len(x_train)}')\n",
    "    print(f'len(valid): {len(x_valid)}')\n",
    "\n",
    "    histories = []\n",
    "    metrics = [Accuracy()]\n",
    "    loss = 'binary_crossentropy'\n",
    "    metric_to_monitor = 'val_accuracy'\n",
    "\n",
    "    optimizer = Adam()\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "    verbosity = 0\n",
    "    # Generators\n",
    "    train_generator = AugmentedImageSequenceFromNpy(x=x_train, y=y_train, batch_size=batch_size,\n",
    "                                                    augmentations=AUGMENTATIONS_TRAIN, stats=stats)\n",
    "\n",
    "    valid_generator = AugmentedImageSequenceFromNpy(x=x_valid, y=y_valid, batch_size=batch_size,\n",
    "                                                    augmentations=AUGMENTATIONS_TEST, stats=stats)\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=metric_to_monitor, patience=early_stopping_patience, verbose=verbosity),\n",
    "        ReduceLROnPlateau(monitor=metric_to_monitor, factor=0.5, patience=early_stopping_patience, min_lr=0.000001),\n",
    "        TensorBoard(gcs_log_dir, histogram_freq=1),\n",
    "        ModelCheckpointGCS(filepath=model_and_metadata_filepath, gcs_filepath=gcs_model_and_metadata_filepath, \n",
    "                           gcs_bucket=bucket, model_metadata=model_base_metadata, monitor=metric_to_monitor, \n",
    "                           verbose=verbosity)\n",
    "    ]\n",
    "\n",
    "    history = model.fit(train_generator, initial_epoch=int(model_base_metadata['epoch']) + 1,\n",
    "                                  epochs=n_epochs,\n",
    "                                  steps_per_epoch=len(train_generator),\n",
    "                                  callbacks=callbacks,\n",
    "                                  validation_data=valid_generator, validation_steps=len(valid_generator),\n",
    "                                  shuffle=True, verbose=1)\n",
    "\n",
    "    actual_y_train, pred_y_train = train_generator.get_predictions(model)\n",
    "    actual_y_valid, pred_y_valid = valid_generator.get_predictions(model)\n",
    "\n",
    "    metadata_filepath = f\"{model_and_metadata_filepath}_metadata.json\"\n",
    "    with open(metadata_filepath, 'r') as json_file:\n",
    "        best_model_metadata = json.load(json_file)\n",
    "\n",
    "    best_model_metadata.update({\n",
    "        'accuracy_train': sklearn.metrics.accuracy_score(actual_y_train, pred_y_train),\n",
    "        'accuracy_valid': sklearn.metrics.accuracy_score(actual_y_valid, pred_y_valid),\n",
    "        'f1_score_train': sklearn.metrics.f1_score(actual_y_train, pred_y_train),\n",
    "        'f1_score_valid': sklearn.metrics.f1_score(actual_y_valid, pred_y_valid),\n",
    "        'confusion_matrix': numpy_to_json(sklearn.metrics.confusion_matrix(actual_y_valid, pred_y_valid)),\n",
    "        'precision_recall_curve': sklearn_precision_recall_curve_to_dict(\n",
    "            sklearn.metrics.precision_recall_curve(actual_y_valid, pred_y_valid)),\n",
    "    })\n",
    "\n",
    "    with open(metadata_filepath, 'w+') as json_file:\n",
    "        json.dump(best_model_metadata, json_file)\n",
    "\n",
    "    blob = bucket.blob(f\"{gcs_model_and_metadata_filepath}_metadata.json\")\n",
    "    blob.upload_from_filename(metadata_filepath)\n",
    "\n",
    "    # Attempt to avoid memory leaks\n",
    "    del train_generator\n",
    "    del valid_generator\n",
    "    gc.collect()\n",
    "\n",
    "# if os.environ.get(\"SHOULD_PREDICT\", \"True\") == \"True\":\n",
    "#     pred_test_labels = predict(model=model, weight_dir=model_path, x=x_test, batch_size=batch_size, n_classes=n_classes)\n",
    "#     clf_report = classification_report(y_test_labels, pred_test_labels, target_names=classes)\n",
    "#     print(clf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previously trained model.\n",
      "Resuming training at epoch 1\n",
      "len(train): 100\n",
      "len(valid): 100\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1 steps, validate for 1 steps\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 21s 21s/step - loss: 0.9972 - accuracy: 0.0400 - val_loss: 3.9282 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/3\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (3.434725). Check your callbacks.\n",
      "1/1 [==============================] - 10s 10s/step - loss: 4.1898 - accuracy: 0.0000e+00 - val_loss: 0.9662 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training for experiment cloud_and_shadow_basic_cnn_2020_1_31\n",
      "Finished training for experiment cloud_and_shadow_basic_cnn_2020_1_31\n"
     ]
    }
   ],
   "source": [
    "experiment_name = f\"{project_name}_basic_cnn_2020_1_31\"\n",
    "train_keras_model(bucket=bucket, model_dir=model_dir, gcs_model_dir=gcs_model_dir, gcs_log_dir=gcs_log_dir, \n",
    "                  experiment_name=experiment_name, start_model=basic_cnn_model((120, 120, 3), n_classes=n_classes))\n",
    "logger.info(f\"Finished training for experiment {experiment_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previously trained model.\n",
      "Resuming training at epoch 1\n",
      "len(train): 100\n",
      "len(valid): 100\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1 steps, validate for 1 steps\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 20s 20s/step - loss: 1.4209 - accuracy: 0.0400 - val_loss: 5.7233 - val_accuracy: 0.0100\n",
      "Epoch 3/3\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (3.772809). Check your callbacks.\n",
      "1/1 [==============================] - 12s 12s/step - loss: 0.6108 - accuracy: 0.0300 - val_loss: 8.5210 - val_accuracy: 0.0000e+00\n",
      "Finished training for experiment cloud_and_shadow_basic_cnn_best_practices_2020_1_31\n",
      "Finished training for experiment cloud_and_shadow_basic_cnn_best_practices_2020_1_31\n"
     ]
    }
   ],
   "source": [
    "experiment_name = f\"{project_name}_basic_cnn_best_practices_2020_1_31\"\n",
    "train_keras_model(bucket=bucket, model_dir=model_dir, gcs_model_dir=gcs_model_dir, gcs_log_dir=gcs_log_dir, \n",
    "                  experiment_name=experiment_name, start_model=basic_cnn_model_with_best_practices((120, 120, 3), n_classes=n_classes))\n",
    "logger.info(f\"Finished training for experiment {experiment_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# test_generator = AugmentedImageSequenceFromNpy(x=x_test, y=None, batch_size=batch_size,\n",
    "#                                                         augmentations=AUGMENTATIONS_TEST)\n",
    "# y_pred = model.predict(test_generator)\n",
    "# y_pred_binary = [0 if pred < .5 else 1 for pred in y_pred]\n",
    "# clf = classification_report(y_test, y_pred_binary,  target_names=['has_clouds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pd.DataFrame(y_pred)[0].unique())\n",
    "# print(pd.DataFrame(y_pred_binary)[0].unique())\n",
    "# pd.DataFrame(y_pred)[0].unique()\n",
    "# pd.DataFrame(y_pred_binary)[0].unique()\n",
    "# full_histories = join_histories(histories)\n",
    "# graph_model_history(full_histories)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

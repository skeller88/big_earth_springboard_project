{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "from joblib import dump, load\n",
    "import random\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "\n",
    "from albumentations import (\n",
    "    Compose, Flip, Rotate\n",
    ")\n",
    "\n",
    "import cv2\n",
    "import dask\n",
    "import dask.array as da\n",
    "\n",
    "from google.cloud import storage\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support, precision_score, precision_recall_curve\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import inception_v3, resnet_v2\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "\n",
    "from data_engineering.dask_image_stats_collector import stats_for_numpy_images \n",
    "from data_science.graph_utils import graph_model_history\n",
    "from data_science.keras.model_checkpoint_gcs import ModelCheckpointGCS\n",
    "from data_science.keras.cnn_models import basic_cnn_model, basic_cnn_model_with_regularization, pretrained_model\n",
    "from data_science.serialization_utils import numpy_to_json, sklearn_precision_recall_curve_to_dict\n",
    "from data_science.sklearn_batch_generator import SklearnBatchGenerator\n",
    "from data_science.train import get_model_and_metadata_from_gcs, train_keras_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tf.test.gpu_device_name())\n",
    "print(tf.test.is_built_with_gpu_support())\n",
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BigEarthNet-v1.0',\n",
       " 'google_automl_cloud_and_shadow_dataset_small_augmented_test.csv',\n",
       " '.DS_Store',\n",
       " 'google_automl_cloud_and_shadow_dataset_small.csv',\n",
       " 'cloud_and_shadow_stats.csv',\n",
       " 'npy_files',\n",
       " 'patches_with_cloud_and_shadow.csv',\n",
       " 'patches_with_seasonal_snow.csv',\n",
       " 'BigEarthNet-v1.0.tar.gz',\n",
       " 'model',\n",
       " 'png_image_files_augmented',\n",
       " 'google_automl_cloud_and_shadow_dataset_small_augmented.csv',\n",
       " 'google_automl_cloud_and_shadow_dataset.csv',\n",
       " 'metadata',\n",
       " 'png_image_files',\n",
       " 'npy_image_files']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = '/home/jovyan/work/data/big_earth'\n",
    "os.listdir(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/app/.gcs/big-earth-252219-fb2e5c109f78.json'\n",
    "gcs_client = storage.Client()\n",
    "bucket = gcs_client.bucket(\"big_earth\")\n",
    "\n",
    "n_classes = 1\n",
    "n_epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "early_stopping_patience = 6\n",
    "use_small_dataset = False\n",
    "use_random_small_dataset = False\n",
    "\n",
    "project_name = \"cloud_and_shadow\"\n",
    "model_dir = os.path.join(root, \"model/models\")\n",
    "log_dir = os.path.join(root, \"model/logs\")\n",
    "\n",
    "gcs_hyperparameter_opt_record_dir = \"model/hyperparameter_opt_record\"\n",
    "hyperparameter_opt_record_dir = os.path.join(root, gcs_hyperparameter_opt_record_dir)\n",
    "# blob prefix\n",
    "gcs_model_dir = \"model/models\"\n",
    "# tensorboard\n",
    "gcs_log_dir = \"gs://big_earth/model/logs\"\n",
    "\n",
    "\n",
    "for directory in [log_dir, model_dir, hyperparameter_opt_record_dir]:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 44)\n",
      "(1,)\n",
      "1907 253 240\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# np.array(json.loads(df['binarized_labels'].iloc[0])).shape\n",
    "\n",
    "def prepare_data(df):\n",
    "    df['has_cloud_and_shadow_target'] = df['has_cloud_and_shadow_target'].apply(lambda x: np.array(json.loads(x)))\n",
    "    df['binarized_labels'] = df['binarized_labels'].apply(lambda x: np.array(json.loads(x)))    \n",
    "    df['image_path'] = root + \"/npy_image_files/\" + df['image_prefix'] + \".npy\"\n",
    "    return df\n",
    "\n",
    "df = pd.read_csv(root + \"/metadata/metadata.csv\")\n",
    "df = prepare_data(df)\n",
    "print(df['binarized_labels'].iloc[0].shape)\n",
    "print(df['has_cloud_and_shadow_target'].iloc[0].shape)\n",
    "df = df.set_index('image_prefix', drop=False)\n",
    "\n",
    "google_automl_dataset = pd.read_csv( '/app/data_science/google_automl_cloud_and_shadow_dataset_small.csv')\n",
    "google_automl_dataset['image_prefix'] = google_automl_dataset['gcs_uri'].str.split('/').apply(lambda x: x[-1].replace(\".png\", \"\"))\n",
    "google_automl_dataset = google_automl_dataset.set_index('image_prefix', drop=False)\n",
    "\n",
    "train = df.loc[google_automl_dataset[google_automl_dataset['set'] == 'TRAIN'].index]\n",
    "valid = df.loc[google_automl_dataset[google_automl_dataset['set'] == 'VALIDATION'].index]\n",
    "test = df.loc[google_automl_dataset[google_automl_dataset['set'] == 'TEST'].index]\n",
    "\n",
    "print(len(train), len(valid), len(test))\n",
    "print(len(train) + len(valid) + len(test) == len(google_automl_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(root + '/cloud_and_shadow_stats.csv'):\n",
    "    all_stats = pd.read_csv(root + '/cloud_and_shadow_stats.csv')\n",
    "else:\n",
    "    stat_list = []\n",
    "    npy_image_dir = root + \"/npy_image_files\"\n",
    "    npy_files = [npy_image_dir + \"/\" + file + \".npy\" for file in train['image_prefix'].values]\n",
    "    start = time.time()\n",
    "    stats = stats_for_numpy_images(npy_files,  use_test_data=False)\n",
    "    stats['data'] = 'all'\n",
    "    stat_list.append(stats)\n",
    "    \n",
    "    # get stats per class\n",
    "    no_cloud = train[train['has_cloud_and_shadow'] == 0]\n",
    "    cloud = train[train['has_cloud_and_shadow'] == 1]\n",
    "    print(len(no_cloud), len(cloud))\n",
    "\n",
    "    for name, data in [('no_cloud', no_cloud), ('cloud', cloud)]:\n",
    "        npy_files = [npy_image_dir + \"/\" + file + \".npy\" for file in data['image_prefix'].values]\n",
    "        stats = stats_for_numpy_images(npy_files,  use_test_data=False)\n",
    "        stats['data'] = name\n",
    "        stat_list.append(stats)    \n",
    "    \n",
    "    all_stats = pd.concat(stat_list)\n",
    "    all_stats['band'] = all_stats.index\n",
    "    all_stats = all_stats.reset_index()  \n",
    "    all_stats = all_stats.drop('index', axis=1)    \n",
    "    all_stats.to_csv(root + '/cloud_and_shadow_stats.csv', index=False)\n",
    "        \n",
    "    print(f'stats computed in {time.time() - start}')\n",
    "\n",
    "band_stats = all_stats[all_stats['data'] == 'all']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1907, 1) (1,)\n",
      "(1907, 1) (1,)\n"
     ]
    }
   ],
   "source": [
    "x_train = train['image_path'].values\n",
    "x_valid = valid['image_path'].values\n",
    "x_test = test['image_path'].values\n",
    "\n",
    "target = 'has_cloud_and_shadow_target'\n",
    "y_train = np.stack(train[target].values)\n",
    "y_valid = np.stack(valid[target].values)\n",
    "y_test = np.stack(test[target].values)\n",
    "\n",
    "print(y_train.shape, y_train[0].shape)\n",
    "\n",
    "if use_small_dataset:\n",
    "    size = batch_size\n",
    "    n_epochs = 3\n",
    "    x_train = np.concatenate([x_train[:size], x_train[-size:]])\n",
    "    x_valid = np.concatenate([x_valid[:size], x_valid[-size:]])\n",
    "    x_test = np.concatenate([x_test[:size], x_test[-size:]])\n",
    "\n",
    "    y_train = np.concatenate([y_train[:size], y_train[-size:]])\n",
    "    y_valid = np.concatenate([y_valid[:size], y_valid[-size:]])\n",
    "    y_test = np.concatenate([y_test[:size], y_test[-size:]])\n",
    "elif use_random_small_dataset:\n",
    "    shape = (100, 1)\n",
    "    x_train = np.concatenate([np.ones(shape), np.zeros(shape)])\n",
    "    y_train = np.concatenate([np.ones(shape), np.zeros(shape)])\n",
    "\n",
    "    x_valid = np.concatenate([np.ones(shape), np.zeros(shape)])\n",
    "    y_valid = np.concatenate([np.ones(shape), np.zeros(shape)])\n",
    "\n",
    "    y_train = np.random.randint(0, 2, (len(train), 44))\n",
    "    y_valid = np.random.randint(0, 2, (len(valid), 44))\n",
    "    y_test = np.random.randint(0, 2, (len(test), 44))\n",
    "    y_test_labels = test['labels'].values\n",
    "\n",
    "print(y_train.shape, y_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model blob.\n",
      "len(train): 10\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 21s 21s/step - loss: 0.6983 - accuracy: 0.8000\n",
      "Epoch 2/2\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (6.258988). Check your callbacks.\n",
      "1/1 [==============================] - 27s 27s/step - loss: 2.4259e-06 - accuracy: 1.0000\n",
      "Downloading model blob.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/env/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fb946626518>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Overfit on all training data\n",
    "model = basic_cnn_model((120, 120, 3), n_classes)\n",
    "experiment_name = f\"{project_name}_basic_cnn_2020_1_31_test\"\n",
    "\n",
    "result = train_keras_model(\n",
    "    random_seed=random_seed,\n",
    "    x_train=x_train[:10], y_train=y_train[:10], x_valid=None, y_valid=None, image_augmentations=None, image_processor=None,\n",
    "    band_stats=band_stats,\n",
    "    bucket=bucket, model_dir=model_dir, gcs_model_dir=gcs_model_dir, gcs_log_dir=gcs_log_dir, \n",
    "    experiment_name=experiment_name, start_model=model, should_train_from_scratch=True, optimizer=Adam, lr=3e-4,\n",
    "    should_upload_to_gcs=True,\n",
    "    n_epochs=2, early_stopping_patience=10)\n",
    "pd.DataFrame(result['y_pred_probs_train']).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 7.061103\n",
      "val_accuracy 0.541501976284585\n"
     ]
    }
   ],
   "source": [
    "valid_generator = get_image_dataset(x=x_valid, y=y_valid, augmentations=None, image_processor=None,\n",
    "                                    band_stats=band_stats, batch_size=batch_size)\n",
    "actual_y_valid, pred_y_valid, pred_y_valid_probs = get_predictions_for_dataset(valid_generator, model) \n",
    "print('val_loss', binary_crossentropy(actual_y_valid, pred_y_valid_probs).numpy())\n",
    "print('val_accuracy', accuracy_score(actual_y_valid, pred_y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model blob.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-744e23e39c6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mexperiment_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperiment_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_train_from_scratch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mshould_upload_to_gcs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     n_epochs=200, early_stopping_patience=30)\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_pred_probs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-8e51d7281564>\u001b[0m in \u001b[0;36mtrain_keras_model\u001b[0;34m(random_seed, x_train, y_train, x_valid, y_valid, band_stats, image_augmentations, image_processor, bucket, model_dir, gcs_model_dir, gcs_log_dir, should_upload_to_gcs, experiment_name, start_model, should_train_from_scratch, optimizer, lr, batch_size, n_epochs, early_stopping_patience, metric_to_monitor)\u001b[0m\n\u001b[1;32m     23\u001b[0m                       early_stopping_patience=6, metric_to_monitor='accuracy'):\n\u001b[1;32m     24\u001b[0m     model, model_base_metadata = get_model_and_metadata_from_gcs(bucket, model_dir, \"h5\", load_model, gcs_model_dir,\n\u001b[0;32m---> 25\u001b[0;31m                                                                  experiment_name)\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mmodel_and_metadata_filepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/data_science/train.py\u001b[0m in \u001b[0;36mget_model_and_metadata_from_gcs\u001b[0;34m(bucket, model_dir, model_file_ext, model_load_func, gcs_model_dir, experiment_name)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mmodel_metadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_metadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mgcs_model_blob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_to_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_load_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/env/lib/python3.6/site-packages/google/cloud/storage/blob.py\u001b[0m in \u001b[0;36mdownload_to_filename\u001b[0;34m(self, filename, client, start, end, raw_download)\u001b[0m\n\u001b[1;32m    755\u001b[0m                     \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m                     \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m                     \u001b[0mraw_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraw_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m                 )\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mresumable_media\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataCorruption\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/env/lib/python3.6/site-packages/google/cloud/storage/blob.py\u001b[0m in \u001b[0;36mdownload_to_file\u001b[0;34m(self, file_obj, client, start, end, raw_download)\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m             self._do_download(\n\u001b[0;32m--> 717\u001b[0;31m                 \u001b[0mtransport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m             )\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mresumable_media\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidResponse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/env/lib/python3.6/site-packages/google/cloud/storage/blob.py\u001b[0m in \u001b[0;36m_do_download\u001b[0;34m(self, transport, file_obj, download_url, headers, start, end, raw_download)\u001b[0m\n\u001b[1;32m    639\u001b[0m                 \u001b[0mdownload_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             )\n\u001b[0;32m--> 641\u001b[0;31m             \u001b[0mdownload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/env/lib/python3.6/site-packages/google/resumable_media/requests/download.py\u001b[0m in \u001b[0;36mconsume\u001b[0;34m(self, transport)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_to_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/env/lib/python3.6/site-packages/google/resumable_media/requests/download.py\u001b[0m in \u001b[0;36m_write_to_stream\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_helpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SINGLE_GET_CHUNK_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_unicode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             )\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbody_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mlocal_hash\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/env/lib/python3.6/site-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/env/lib/python3.6/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/env/lib/python3.6/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m                 if (\n\u001b[1;32m    509\u001b[0m                     \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/env/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/env/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/env/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/env/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1010\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1012\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/env/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/env/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \"\"\"\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start basic_cnn_model_with_regularization by adding batch normalization and validation set \n",
    "model = basic_cnn_model_with_regularization((120, 120, 3), n_classes)\n",
    "experiment_name = f\"{project_name}_basic_cnn_with_regularization_2020_1_31\"\n",
    "result = train_keras_model(\n",
    "    random_seed=random_seed,\n",
    "    x_train=x_train, y_train=y_train, x_valid=None, y_valid=None, image_augmentations=None, image_processor=None,\n",
    "    band_stats=band_stats,\n",
    "    bucket=bucket, model_dir=model_dir, gcs_model_dir=gcs_model_dir, gcs_log_dir=gcs_log_dir, \n",
    "    experiment_name=experiment_name, start_model=model, should_train_from_scratch=False, optimizer=Adam, lr=3e-4,\n",
    "    should_upload_to_gcs=True,\n",
    "    n_epochs=200, early_stopping_patience=30)\n",
    "pd.DataFrame(result['y_pred_probs']).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_model_history(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations_train = Compose([\n",
    "    Flip(p=0.5),\n",
    "    Rotate(limit=(0, 360), p=0.5),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model blob.\n",
      "Resuming training at epoch 41\n",
      "len(train): 1907\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n",
      "Epoch 42/100\n",
      "      7/Unknown - 27s 4s/step - loss: 0.1788 - accuracy: 0.9342"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3739fb6185e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mexperiment_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperiment_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_train_from_scratch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mshould_upload_to_gcs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     n_epochs=100, early_stopping_patience=30)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_pred_probs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/data_science/train.py\u001b[0m in \u001b[0;36mtrain_keras_model\u001b[0;34m(random_seed, x_train, y_train, x_valid, y_valid, band_stats, image_augmentations, image_processor, bucket, model_dir, gcs_model_dir, gcs_log_dir, should_upload_to_gcs, experiment_name, start_model, should_train_from_scratch, optimizer, lr, batch_size, n_epochs, early_stopping_patience, metric_to_monitor)\u001b[0m\n\u001b[1;32m    108\u001b[0m                                   \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                                   shuffle=True, verbose=1)\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;31m# load the best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/app/env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[1;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[1;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[1;32m    975\u001b[0m                  outputs['metrics'])\n",
      "\u001b[0;32m/app/env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[1;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[1;32m    312\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    266\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backwards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaled_total_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m           \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_total_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m           if isinstance(model.optimizer,\n\u001b[1;32m    270\u001b[0m                         loss_scale_optimizer.LossScaleOptimizer):\n",
      "\u001b[0;32m/app/env/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/env/lib/python3.6/site-packages/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m/app/env/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/env/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_FusedBatchNormV3Grad\u001b[0;34m(op, *grad)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegisterGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FusedBatchNormV3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_FusedBatchNormV3Grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_BaseFusedBatchNormGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/env/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_BaseFusedBatchNormGrad\u001b[0;34m(op, version, *grad)\u001b[0m\n\u001b[1;32m    888\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"reserve_space_3\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m     \u001b[0mpop_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/env/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mfused_batch_norm_grad_v3\u001b[0;34m(y_backprop, x, scale, reserve_space_1, reserve_space_2, reserve_space_3, epsilon, data_format, is_training, name)\u001b[0m\n\u001b[1;32m   4325\u001b[0m         \u001b[0my_backprop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreserve_space_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreserve_space_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4326\u001b[0m         \u001b[0mreserve_space_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"epsilon\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4327\u001b[0;31m         \"is_training\", is_training)\n\u001b[0m\u001b[1;32m   4328\u001b[0m       \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FusedBatchNormGradV3Output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4329\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Add image augmentation\n",
    "model = basic_cnn_model_with_regularization((120, 120, 3), n_classes)\n",
    "experiment_name = f\"{project_name}_basic_cnn_with_regularization_and_aug_2020_1_31\"\n",
    "\n",
    "result = train_keras_model(\n",
    "    random_seed=random_seed,\n",
    "    x_train=x_train, y_train=y_train, x_valid=None, y_valid=None, image_augmentations=augmentations_train, \n",
    "    image_processor=None,\n",
    "    band_stats=band_stats,\n",
    "    bucket=bucket, model_dir=model_dir, gcs_model_dir=gcs_model_dir, gcs_log_dir=gcs_log_dir, \n",
    "    experiment_name=experiment_name, start_model=model, should_train_from_scratch=False, optimizer=Adam, lr=3e-4,\n",
    "    should_upload_to_gcs=True,\n",
    "    n_epochs=100, early_stopping_patience=30)\n",
    "pd.DataFrame(result['y_pred_probs_valid']).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "graph_model_history(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from copy import copy\n",
    "\n",
    "import sklearn\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from data_science.keras.dataset import get_image_dataset, get_predictions_for_dataset\n",
    "from data_science.keras.model_checkpoint_gcs import ModelCheckpointGCS\n",
    "from data_science.serialization_utils import numpy_to_json, sklearn_precision_recall_curve_to_dict\n",
    "\n",
    "\n",
    "def get_model_and_metadata_from_gcs(bucket, model_dir, model_file_ext, model_load_func, gcs_model_dir, experiment_name):\n",
    "    model_and_metadata_filepath = os.path.join(model_dir, experiment_name)\n",
    "    metadata_filepath = f\"{model_and_metadata_filepath}_metadata.json\"\n",
    "    model_filepath = f\"{model_and_metadata_filepath}.{model_file_ext}\"\n",
    "\n",
    "    gcs_model_and_metadata_filepath = os.path.join(gcs_model_dir, experiment_name)\n",
    "    gcs_metadata_filepath = f\"{gcs_model_and_metadata_filepath}_metadata.json\"\n",
    "    gcs_model_filepath = f\"{gcs_model_and_metadata_filepath}.{model_file_ext}\"\n",
    "\n",
    "    gcs_metadata_blob = bucket.blob(gcs_metadata_filepath)\n",
    "    gcs_model_blob = bucket.blob(gcs_model_filepath)\n",
    "\n",
    "    if gcs_metadata_blob.exists():\n",
    "        print('Downloading model blob.')\n",
    "        gcs_metadata_blob.download_to_filename(metadata_filepath)\n",
    "\n",
    "        with open(metadata_filepath, 'r') as json_file:\n",
    "            model_metadata = json.load(json_file)\n",
    "\n",
    "        model_metadata['epoch'] = int(model_metadata['epoch'])\n",
    "\n",
    "        gcs_model_blob.download_to_filename(model_filepath)\n",
    "\n",
    "        model = model_load_func(model_filepath)\n",
    "        return model, model_metadata\n",
    "\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def train_keras_model(*, random_seed, x_train, y_train, x_valid, y_valid, band_stats, image_augmentations,\n",
    "                      image_processor, bucket, model_dir, gcs_model_dir, gcs_log_dir, should_upload_to_gcs,\n",
    "                      experiment_name, model_name, start_model, should_train_from_scratch, optimizer, lr, batch_size=128,\n",
    "                      n_epochs=100,\n",
    "                      early_stopping_patience=6, metric_to_monitor='accuracy', should_return_serializable_metadata=False):\n",
    "    # TODO - deserialize existing model metadata from json\n",
    "    model, model_base_metadata = get_model_and_metadata_from_gcs(bucket, model_dir, \"h5\", load_model, gcs_model_dir,\n",
    "                                                                 experiment_name)\n",
    "\n",
    "    model_and_metadata_filepath = os.path.join(model_dir, experiment_name)\n",
    "    gcs_model_and_metadata_filepath = os.path.join(gcs_model_dir, experiment_name)\n",
    "    gcs_log_dir = os.path.join(gcs_log_dir, experiment_name)\n",
    "\n",
    "    train_start_time = time.time()\n",
    "    if model is None or should_train_from_scratch:\n",
    "        now = datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M\")\n",
    "        model = start_model\n",
    "        model_base_metadata = {\n",
    "            'data': 'train_valid_google_automl_cloud_and_shadow_dataset_small.csv',\n",
    "            'data_prep': 'normalization_augmentation',\n",
    "            'experiment_name': experiment_name,\n",
    "            'experiment_start_time': now,\n",
    "            'model': model_name,\n",
    "            'random_state': random_seed,\n",
    "            # so that initial_epoch is 0\n",
    "            'epoch_with_best_model': -1,\n",
    "            'epochs_trained': -1,\n",
    "            'optimizer': optimizer.__name__,\n",
    "            'n_epochs': n_epochs,\n",
    "            'early_stopping_patience': early_stopping_patience,\n",
    "            'learning_rate': lr,\n",
    "            'batch_size': batch_size\n",
    "        }\n",
    "    else:\n",
    "        print('Resuming training at epoch', int(model_base_metadata['epochs_trained']) + 1)\n",
    "\n",
    "    print(f'len(train): {len(x_train)}')\n",
    "\n",
    "    if x_valid is not None:\n",
    "        print(f'len(valid): {len(x_valid)}')\n",
    "        metric_to_monitor = f'val_{metric_to_monitor}'\n",
    "        valid_generator = get_image_dataset(x=x_valid, y=y_valid, augmentations=image_augmentations,\n",
    "                                            image_processor=image_processor,\n",
    "                                            band_stats=band_stats, batch_size=batch_size)\n",
    "    else:\n",
    "        valid_generator = None\n",
    "\n",
    "    metrics = ['accuracy']\n",
    "    loss = 'binary_crossentropy'\n",
    "\n",
    "    optimizer = optimizer(learning_rate=lr)\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "    verbosity = 0\n",
    "    train_generator = get_image_dataset(x=x_train, y=y_train, augmentations=image_augmentations,\n",
    "                                        image_processor=image_processor,\n",
    "                                        band_stats=band_stats, batch_size=batch_size)\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=metric_to_monitor, patience=early_stopping_patience, verbose=verbosity),\n",
    "        ReduceLROnPlateau(monitor=metric_to_monitor, factor=0.5, patience=early_stopping_patience, min_lr=1e-6),\n",
    "        TensorBoard(gcs_log_dir, histogram_freq=1),\n",
    "    ]\n",
    "\n",
    "    if should_upload_to_gcs:\n",
    "        callbacks.append(\n",
    "            ModelCheckpointGCS(filepath=model_and_metadata_filepath, gcs_filepath=gcs_model_and_metadata_filepath,\n",
    "                               gcs_bucket=bucket, model_metadata=model_base_metadata, monitor=metric_to_monitor,\n",
    "                               verbose=verbosity))\n",
    "\n",
    "    history = model.fit_generator(train_generator, initial_epoch=int(model_base_metadata['epochs_trained']) + 1,\n",
    "                                  epochs=n_epochs,\n",
    "                                  callbacks=callbacks,\n",
    "                                  validation_data=valid_generator,\n",
    "                                  shuffle=True, verbose=1)\n",
    "\n",
    "    # load the best model\n",
    "    if should_upload_to_gcs:\n",
    "        best_model, best_model_metadata = get_model_and_metadata_from_gcs(bucket, model_dir, \"h5\", load_model,\n",
    "                                                                          gcs_model_dir,\n",
    "                                                                          experiment_name)\n",
    "    else:\n",
    "        best_model = model\n",
    "        best_model_metadata = model_base_metadata\n",
    "\n",
    "    y_actual_train, y_pred_train, y_pred_probs_train = get_predictions_for_dataset(train_generator, best_model)\n",
    "    train_loss = binary_crossentropy(y_actual_train, y_pred_probs_train).numpy().tolist()\n",
    "\n",
    "    # add more stats\n",
    "    best_model_metadata.update({\n",
    "        'history': history.history,\n",
    "        'accuracy_train': sklearn.metrics.accuracy_score(y_actual_train, y_pred_train),\n",
    "        'f1_score_train': sklearn.metrics.f1_score(y_actual_train, y_pred_train),\n",
    "        'train_loss': train_loss,\n",
    "        'loss': train_loss,\n",
    "        'y_actual_train': y_actual_train,\n",
    "        'y_pred_train': y_pred_train,\n",
    "        'y_pred_probs_train': y_pred_probs_train,\n",
    "        'epochs_trained': len(history.history),\n",
    "        'elapsed_train_time': time.time() - train_start_time\n",
    "    })\n",
    "\n",
    "    if valid_generator is not None:\n",
    "        y_actual_valid, y_pred_valid, y_pred_probs_valid = get_predictions_for_dataset(valid_generator, best_model)\n",
    "        valid_loss = binary_crossentropy(y_actual_valid, y_pred_probs_valid).numpy().tolist()\n",
    "        best_model_metadata.update({\n",
    "            'accuracy_valid': sklearn.metrics.accuracy_score(y_actual_valid, y_pred_valid),\n",
    "            'f1_score_valid': sklearn.metrics.f1_score(y_actual_valid, y_pred_valid),\n",
    "            'confusion_matrix': sklearn.metrics.confusion_matrix(y_actual_valid, y_pred_valid),\n",
    "            'precision_recall_curve': sklearn.metrics.precision_recall_curve(y_actual_valid, y_pred_valid),\n",
    "            'y_actual_valid': y_actual_valid,\n",
    "            'y_pred_valid': y_pred_valid,\n",
    "            'y_pred_probs_valid': y_pred_probs_valid,\n",
    "            'loss': valid_loss\n",
    "        })\n",
    "\n",
    "    serializable_metadata = copy(best_model_metadata)\n",
    "    json_serializable_history = {}\n",
    "    for k, v in history.history.items():\n",
    "        # TODO - decide whether to keep these variables because we can get them from the model?\n",
    "        # if k.contains(\"y_actual\") or k.contains(\"y_pred\"):\n",
    "        #     continue\n",
    "        json_serializable_history[k] = list(map(float, v))\n",
    "    serializable_metadata['history'] = json_serializable_history\n",
    "\n",
    "    if valid_generator is not None:\n",
    "        serializable_metadata['confusion_matrix'] = numpy_to_json(serializable_metadata['confusion_matrix'])\n",
    "        serializable_metadata['precision_recall_curve'] = sklearn_precision_recall_curve_to_dict(\n",
    "            serializable_metadata['precision_recall_curve'])\n",
    "\n",
    "    datasets = [\"train\", \"valid\"] if valid_generator is not None else [\"train\"]\n",
    "    for dataset in datasets:\n",
    "        for variable in [\"y_actual\", \"y_pred\", \"y_pred_probs\"]:\n",
    "            serializable_metadata[f\"{variable}_{dataset}\"] = serializable_metadata[f\"{variable}_{dataset}\"].tolist()\n",
    "\n",
    "    metadata_filepath = f\"{model_and_metadata_filepath}_metadata.json\"\n",
    "    if should_upload_to_gcs:\n",
    "        with open(metadata_filepath, 'w+') as json_file:\n",
    "            json.dump(serializable_metadata, json_file)\n",
    "\n",
    "        blob = bucket.blob(f\"{gcs_model_and_metadata_filepath}_metadata.json\")\n",
    "        blob.upload_from_filename(metadata_filepath)\n",
    "\n",
    "    if should_return_serializable_metadata:\n",
    "        return serializable_metadata\n",
    "    return best_model_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train): 10                                       \n",
      "len(valid): 10                                       \n",
      "  0%|          | 0/2 [00:00<?, ?trial/s, best loss=?]WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n",
      "Epoch 1/2                                            \n",
      "      1/Unknown                                      \n",
      " - 1s 682ms/step - loss: 1.0049 - accuracy: 0.5000   \n",
      "                                                     \n",
      "1/1 [==============================]                 \n",
      " - 37s 37s/step - loss: 1.0049 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 2/2                                            \n",
      "  0%|          | 0/2 [00:49<?, ?trial/s, best loss=?]WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (10.669321). Check your callbacks.\n",
      "1/1 [==============================]                 \n",
      " - 46s 46s/step - loss: 1.0369 - accuracy: 0.6000 - val_loss: 4.4858 - val_accuracy: 0.1000\n",
      "\n",
      "Downloading model blob.                              \n",
      "  0%|          | 0/2 [01:35<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/env/lib/python3.6/site-packages/sklearn/metrics/_ranking.py:677: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train): 10                                                                \n",
      "len(valid): 10                                                                \n",
      " 50%|█████     | 1/2 [01:44<01:43, 103.79s/trial, best loss: 4.55850887298584]WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n",
      "Epoch 1/2                                                                     \n",
      "      1/Unknown                                                               \n",
      " - 1s 652ms/step - loss: 0.8568 - accuracy: 0.5000                            \n",
      "                                                                              \n",
      "1/1 [==============================]                                          \n",
      " - 38s 38s/step - loss: 0.8568 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 2/2                                                                     \n",
      " 50%|█████     | 1/2 [02:31<01:43, 103.79s/trial, best loss: 4.55850887298584]WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (11.405980). Check your callbacks.\n",
      "1/1 [==============================]                                          \n",
      " - 21s 21s/step - loss: 1.4393 - accuracy: 0.5000 - val_loss: 11.4033 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Downloading model blob.                                                       \n",
      " 50%|█████     | 1/2 [02:52<01:43, 103.79s/trial, best loss: 4.55850887298584]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/env/lib/python3.6/site-packages/sklearn/metrics/_ranking.py:677: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [03:01<00:00, 90.50s/trial, best loss: 4.55850887298584] \n"
     ]
    }
   ],
   "source": [
    "# Try hyperparameter optimization\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll.base import scope\n",
    "\n",
    "def int_quinform(label, start, end):\n",
    "    scope.int(hp.quinform(lable, start, end))\n",
    "    \n",
    "\n",
    "def optimize():\n",
    "    def train_keras_with_hyperopt_params(params):\n",
    "        model = basic_cnn_model_with_regularization((120, 120, 3), n_classes)\n",
    "        experiment_name = (f\"{project_name}_basic_cnn_early_stopping_patience_{params['early_stopping_patience']}_lr\"\n",
    "                           f\"_{round(params['learning_rate'], 8)}_optimizer\"\n",
    "                           f\"_{params['optimizer'][0]}_2020_2_08\")\n",
    "        result = train_keras_model(\n",
    "            random_seed=random_seed, x_train=x_train[:10], y_train=y_train[:10], x_valid=x_valid[:10], y_valid=y_valid[:10],\n",
    "            image_augmentations=augmentations_train, image_processor=None, band_stats=band_stats, bucket=bucket, model_dir=model_dir,\n",
    "            gcs_model_dir=gcs_model_dir, gcs_log_dir=gcs_log_dir, experiment_name=experiment_name, start_model=model,\n",
    "            model_name='cnn_bn',\n",
    "            should_train_from_scratch=True, optimizer=params['optimizer'][1], lr=params['learning_rate'],\n",
    "            should_upload_to_gcs=True, n_epochs=2, early_stopping_patience=params['early_stopping_patience'],\n",
    "            should_return_serializable_metadata=True\n",
    "        )\n",
    "\n",
    "        result['status'] = STATUS_OK\n",
    "        return result\n",
    "\n",
    "    space = {\n",
    "        'learning_rate': hp.loguniform('learning_rate', np.log(1e-5), np.log(1e-2)),\n",
    "        'batch_size': scope.int(hp.quniform('batch_size', 32, 128)),\n",
    "        'optimizer': hp.choice('optimizer', [\n",
    "            ('Adam', Adam), ('SGD', SGD), ('RMSprop', RMSprop)])\n",
    "    }\n",
    "    trials = Trials()\n",
    "    best = fmin(fn=train_keras_with_hyperopt_params,\n",
    "                algo=tpe.suggest,\n",
    "                space=space,\n",
    "                max_evals=2,\n",
    "                trials=trials\n",
    "               )\n",
    "\n",
    "    return best, trials, space\n",
    "\n",
    "\n",
    "best, trials = optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "space = {\n",
    "    'learning_rate': {\n",
    "        'start': np.log(1e-5),\n",
    "        'end': np.log(1e-2),\n",
    "        'distribution': 'hp.loguniform',\n",
    "        \n",
    "    },\n",
    "    'batch_size': {\n",
    "        'start': 32,\n",
    "        'end': 128,\n",
    "        'distribution': 'scope.int(hp.quniform)', \n",
    "    },    \n",
    "    'optimizer': {\n",
    "        'values': ['Adam', 'SGD', 'RMSprop'],\n",
    "        'distribution': 'hp.choice',\n",
    "    },   \n",
    "}\n",
    "\n",
    "serializable_trials = []\n",
    "for trial in trials.trials:\n",
    "    serializable_trials.append({\n",
    "        'trial_num': trial['tid'],\n",
    "        'trial_state': trial['state'],\n",
    "        'result': trial['result']\n",
    "    })\n",
    "\n",
    "    hyperparameter_opt_record = {\n",
    "    'trials': serializable_trials,\n",
    "    'space': space\n",
    "}\n",
    "\n",
    "hyperparameter_opt_name = 'learning_rate_batch_size_optimizer_2020_2_12.json'\n",
    "\n",
    "hyperparameter_opt_record_filepath = os.path.join(hyperparameter_opt_record_dir, hyperparameter_opt_name)\n",
    "\n",
    "with open(hyperparameter_opt_record_filepath, 'w+') as json_file:\n",
    "    json.dump(hyperparameter_opt_record, json_file)\n",
    "\n",
    "blob = bucket.blob(f\"{gcs_hyperparameter_opt_record_dir}/{hyperparameter_opt_name}\")\n",
    "blob.upload_from_filename(hyperparameter_opt_record_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-537d837efd1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# https://keras.io/applications/#inceptionv3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minception_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m299\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m299\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minception_v3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInceptionV3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minception_input_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mexperiment_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{project_name}_inception_v3_with_aug_2020_2_08\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/data_science/keras/cnn_models.py\u001b[0m in \u001b[0;36mpretrained_model\u001b[0;34m(base_model_class, input_shape, output_shape)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \"\"\"\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# from https://www.kaggle.com/sashakorekov/end-to-end-resnet50-with-tta-lb-0-93#L321\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/env/lib/python3.6/site-packages/tensorflow_core/python/keras/applications/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'utils'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbase_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/env/lib/python3.6/site-packages/tensorflow_core/python/keras/applications/inception_v3.py\u001b[0m in \u001b[0;36mInceptionV3\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mkeras_modules_injection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mInceptionV3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0minception_v3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInceptionV3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/env/lib/python3.6/site-packages/keras_applications/inception_v3.py\u001b[0m in \u001b[0;36mInceptionV3\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mbranch7x7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv2d_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m160\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mbranch7x7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv2d_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbranch7x7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m160\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mbranch7x7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv2d_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbranch7x7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m192\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mbranch7x7dbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv2d_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m160\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/env/lib/python3.6/site-packages/keras_applications/inception_v3.py\u001b[0m in \u001b[0;36mconv2d_bn\u001b[0;34m(x, filters, num_row, num_col, padding, strides, name)\u001b[0m\n\u001b[1;32m     78\u001b[0m         name=conv_name)(x)\n\u001b[1;32m     79\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbn_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbn_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m               with base_layer_utils.autocast_context_manager(\n\u001b[0;32m--> 836\u001b[0;31m                   self._compute_dtype):\n\u001b[0m\u001b[1;32m    837\u001b[0m                 \u001b[0;31m# Add auto_control_deps in V2 when they are not already added by\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m                 \u001b[0;31m# a `tf.function`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36mautocast_context_manager\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mautocast_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m   \"\"\"Returns a context manager to autocast AutoCastVariables.\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Try transfer learning\n",
    "# https://keras.io/applications/#inceptionv3\n",
    "inception_input_shape = (299, 299, 3)\n",
    "model = pretrained_model(inception_v3.InceptionV3, inception_input_shape, n_classes)\n",
    "experiment_name = f\"{project_name}_inception_v3_with_aug_2020_2_08\"\n",
    "\n",
    "def inception_v3_image_processor(image):\n",
    "    resized_img = cv2.resize(image, dsize=(inception_input_shape[0], inception_input_shape[1]), \n",
    "                             interpolation=cv2.INTER_CUBIC)\n",
    "    return inception_v3.preprocess_input(resized_img)\n",
    "\n",
    "\n",
    "result = train_keras_model(\n",
    "    random_seed=random_seed,\n",
    "    x_train=x_train, y_train=y_train, x_valid=None, y_valid=None, image_augmentations=augmentations_train, \n",
    "    image_processor=inception_v3_image_processor,\n",
    "    band_stats=band_stats,\n",
    "    bucket=bucket, model_dir=model_dir, gcs_model_dir=gcs_model_dir, gcs_log_dir=gcs_log_dir, \n",
    "    experiment_name=experiment_name, start_model=model, should_train_from_scratch=True, optimizer=Adam, lr=3e-4,\n",
    "    should_upload_to_gcs=True,\n",
    "    n_epochs=100, early_stopping_patience=30)\n",
    "\n",
    "pd.DataFrame(result['y_pred_probs']).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_model_history(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_v2_input_shape = (224, 224, 3)\n",
    "model = pretrained_model(resnet_v2.InceptionV3, inception_input_shape, n_classes)\n",
    "experiment_name = f\"{project_name}_resnet_v2_with_aug_2020_2_08\"\n",
    "\n",
    "def resnet_v2_image_processor(image):\n",
    "    resized_img = cv2.resize(image, dsize=(resnet_v2_input_shape[0], resnet_v2_input_shape[1]), \n",
    "                             interpolation=cv2.INTER_CUBIC)\n",
    "    return resnet_v2.preprocess_input(resized_img)\n",
    "\n",
    "history, y_actual, y_pred, y_pred_probs = train_keras_model(\n",
    "    random_seed=random_seed,\n",
    "    x_train=x_train, y_train=y_train, x_valid=None, y_valid=None, image_augmentations=augmentations_train, \n",
    "    image_processor=inception_v3_image_processor,\n",
    "    band_stats=band_stats,\n",
    "    bucket=bucket, model_dir=model_dir, gcs_model_dir=gcs_model_dir, gcs_log_dir=gcs_log_dir, \n",
    "    experiment_name=experiment_name, start_model=model, should_train_from_scratch=True, optimizer=Adam, lr=3e-4,\n",
    "    should_upload_to_gcs=True,\n",
    "    n_epochs=100, early_stopping_patience=30)\n",
    "\n",
    "pd.DataFrame(y_pred_probs).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_model_history(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision[:-1]\n",
    "recall = recall[:-1]\n",
    "df = pd.concat([\n",
    "    pd.DataFrame({'stat_name': ['precision' for _ in range(len(precision))],\n",
    "                  'stat_value': precision,\n",
    "                  'threshold': thresholds}),\n",
    "    pd.DataFrame({'stat_name': ['recall' for _ in range(len(precision))],\n",
    "                  'stat_value': recall,\n",
    "                  'threshold': thresholds})    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(x=recall, y=precision, color='darkblue')\n",
    "ax.fill_betweean(recall,precision, color=\"darkblue\", alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(y=df['stat_value'], x=df['threshold'], hue=df['stat_name'], \n",
    "                  palette={'precision': 'red', 'recall': 'blue'})\n",
    "# ax.fill_between(recall,precision, color=\"darkblue\", alpha=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "from joblib import dump, load\n",
    "import random\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "\n",
    "from albumentations import (\n",
    "    Compose, Flip, Rotate\n",
    ")\n",
    "\n",
    "import dask\n",
    "import dask.array as da\n",
    "from google.cloud import storage\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support, precision_score, precision_recall_curve\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "\n",
    "from data_science.augmented_image_sequence_from_npy import AugmentedImageSequenceFromNpy\n",
    "from data_science.keras.model_checkpoint_gcs import ModelCheckpointGCS\n",
    "from data_science.keras.cnn_models import basic_cnn_model, basic_cnn_model_with_best_practices\n",
    "from data_science.serialization_utils import numpy_to_json, sklearn_precision_recall_curve_to_dict\n",
    "from data_science.sklearn_batch_generator import SklearnBatchGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "n_classes = 1\n",
    "n_epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "early_stopping_patience = 6\n",
    "\n",
    "use_small_dataset = True\n",
    "use_random_small_dataset = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/app/.gcs/big-earth-252219-fb2e5c109f78.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pal = sns.color_palette()\n",
    "\n",
    "random_seed = 0\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "root = '/home/jovyan/work/data/big_earth'\n",
    "project_name = \"cloud_and_shadow\"\n",
    "log_dir = os.path.join(root, \"model/logs\")\n",
    "model_dir = os.path.join(root, \"model/models\")\n",
    "gcs_model_dir = \"gcs://big_earth/model/models\"\n",
    "\n",
    "for directory in [log_dir, model_dir]:\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_client = storage.Client()\n",
    "bucket = gcs_client.bucket(\"big_earth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 44)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# np.array(json.loads(df['binarized_labels'].iloc[0])).shape\n",
    "\n",
    "def prepare_data(df):\n",
    "    df['has_cloud_and_shadow_target'] = df['has_cloud_and_shadow_target'].apply(lambda x: np.array(json.loads(x)))\n",
    "    df['binarized_labels'] = df['binarized_labels'].apply(lambda x: np.array(json.loads(x)))    \n",
    "    df['image_path'] = root + \"/npy_image_files/\" + df['image_prefix'] + \".npy\"\n",
    "    return df\n",
    "\n",
    "df = pd.read_csv(root + \"/metadata/metadata.csv\")\n",
    "df = prepare_data(df)\n",
    "print(df['binarized_labels'].iloc[0].shape)\n",
    "print(df['has_cloud_and_shadow_target'].iloc[0].shape)\n",
    "df = df.set_index('image_prefix', drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# has_cloud_and_shadow = df[df['has_cloud_and_shadow'] == 1]\n",
    "# sample_no_cloud_and_shadow = df[df['has_cloud_and_shadow'] == 0].sample(\n",
    "#     n=len(has_cloud_and_shadow), random_state=random_seed)\n",
    "\n",
    "# print(\"len(sample_no_cloud_and_shadow)\", len(sample_no_cloud_and_shadow), \"len(has_cloud_and_shadow)\", \n",
    "#       len(has_cloud_and_shadow))\n",
    "\n",
    "# train, valid, test = balanced_class_train_test_splits(*[sample_no_cloud_and_shadow, has_cloud_and_shadow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1907 253 240\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# len(pd.read_csv(root + '/google_automl_cloud_and_shadow_dataset.csv'))\n",
    "\n",
    "google_automl_dataset = pd.read_csv(root + '/google_automl_cloud_and_shadow_dataset_small.csv')\n",
    "google_automl_dataset['image_prefix'] = google_automl_dataset['gcs_uri'].str.split('/').apply(lambda x: x[-1].replace(\".png\", \"\"))\n",
    "google_automl_dataset = google_automl_dataset.set_index('image_prefix', drop=False)\n",
    "\n",
    "train = df.loc[google_automl_dataset[google_automl_dataset['set'] == 'TRAIN'].index]\n",
    "valid = df.loc[google_automl_dataset[google_automl_dataset['set'] == 'VALIDATION'].index]\n",
    "test = df.loc[google_automl_dataset[google_automl_dataset['set'] == 'TEST'].index]\n",
    "\n",
    "print(len(train), len(valid), len(test))\n",
    "print(len(train) + len(valid) + len(test) == len(google_automl_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# npy_image_dir = root + \"/npy_image_files\"\n",
    "# npy_files = [npy_image_dir + \"/\" + file + \".npy\" for file in train['image_prefix'].values]\n",
    "# start = time.time()\n",
    "# stats = stats_for_numpy_images(npy_files,  use_test_data=False)\n",
    "# stats.to_csv('cloud_and_shadow_stats.csv', index_label='band')\n",
    "# print(time.time() - start)\n",
    "\n",
    "stats = pd.read_csv('cloud_and_shadow_stats.csv', index_col='band')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_histories(histories):\n",
    "    full_history = defaultdict(list)\n",
    "\n",
    "    for history in histories:\n",
    "        for key, value in history.history.items():\n",
    "            full_history[key].extend(value)\n",
    "    return full_history\n",
    "\n",
    "\n",
    "def graph_model_history(history):\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    t = f.suptitle('Basic CNN Performance', fontsize=12)\n",
    "    f.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "    max_epoch = len(history['val_loss']) + 1\n",
    "    epoch_list = list(range(1, max_epoch))\n",
    "    ax1.plot(epoch_list, history['accuracy'], label='Train Accuracy')\n",
    "    ax1.plot(epoch_list, history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax1.set_xticks(np.arange(1, max_epoch, 5))\n",
    "    ax1.set_ylabel('Accuracy Value')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_title('Accuracy')\n",
    "    l1 = ax1.legend(loc=\"best\")\n",
    "\n",
    "    ax2.plot(epoch_list, history['loss'], label='Train Loss')\n",
    "    ax2.plot(epoch_list, history['val_loss'], label='Validation Loss')\n",
    "    ax2.set_xticks(np.arange(1, max_epoch, 5))\n",
    "    ax2.set_ylabel('Loss Value')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_title('Loss')\n",
    "    l2 = ax2.legend(loc=\"best\")\n",
    "\n",
    "\n",
    "def predict(model, model_path, x, batch_size, n_classes):\n",
    "    thresholds = np.array([0.5 for _ in range(n_classes)])\n",
    "    model.load_weights(model_path)\n",
    "\n",
    "    predict_generator = AugmentedImageSequenceFromNpy(x=x, y=None, batch_size=batch_size,\n",
    "                                                        augmentations=AUGMENTATIONS_TEST)\n",
    "    # Generators\n",
    "    pred_test_probs = model.predict_generator(predict_generator)\n",
    "    pred_test_labels = pd.DataFrame(pred_test_probs, columns=classes)\n",
    "    pred_test_labels = pred_test_labels.apply(lambda x: x > thresholds, axis=1)\n",
    "    # Convert boolean predictions to labels\n",
    "    pred_test_lables = pred_test_labels.apply(lambda row: ' '.join(row[row].index), axis=1)\n",
    "\n",
    "    del predict_generator\n",
    "    gc.collect()\n",
    "\n",
    "    return pred_test_labels\n",
    "\n",
    "AUGMENTATIONS_TRAIN = Compose([\n",
    "    Flip(p=0.5),\n",
    "    Rotate(limit=(0, 360), p=0.5)\n",
    "])\n",
    "\n",
    "AUGMENTATIONS_TEST = Compose([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1907, 1) (1,)\n",
      "(100, 120, 120, 3) (100, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train = train['image_path'].values\n",
    "x_valid = valid['image_path'].values\n",
    "x_test = test['image_path'].values\n",
    "\n",
    "target = 'has_cloud_and_shadow_target'\n",
    "y_train = np.stack(train[target].values)\n",
    "y_valid = np.stack(valid[target].values)\n",
    "y_test = np.stack(test[target].values)\n",
    "\n",
    "print(y_train.shape, y_train[0].shape)\n",
    "\n",
    "if use_small_dataset:\n",
    "    n_epochs = 3\n",
    "    x_train = np.concatenate([x_train[:50], x_train[-50:]])\n",
    "    x_valid = np.concatenate([x_valid[:50], x_valid[-50:]])\n",
    "    x_test = np.concatenate([x_test[:50], x_test[-50:]])\n",
    "\n",
    "    y_train = np.concateate([y_train[:50], y_train[-50:]])\n",
    "    y_valid = np.concatenate([y_valid[:50], y_valid[-50:]])\n",
    "    y_test = np.concatenate([y_test[:50], y_test[-50:]])\n",
    "elif use_random_small_dataset:\n",
    "    shape = (100, 1)\n",
    "    x_train = np.concatenate([np.ones(shape), np.zeros(shape)])\n",
    "    y_train = np.concatenate([np.ones(shape), np.zeros(shape)])\n",
    "\n",
    "    x_valid = np.concatenate([np.ones(shape), np.zeros(shape)])\n",
    "    y_valid = np.concatenate([np.ones(shape), np.zeros(shape)])\n",
    "\n",
    "    y_train = np.random.randint(0, 2, (len(train), 44))\n",
    "    y_valid = np.random.randint(0, 2, (len(valid), 44))\n",
    "    y_test = np.random.randint(0, 2, (len(test), 44))\n",
    "    y_test_labels = test['labels'].values\n",
    "\n",
    "a = AugmentedImageSequenceFromNpy(x=x_train, y=y_train,\n",
    "                                  batch_size=batch_size,\n",
    "                                  augmentations=AUGMENTATIONS_TRAIN, stats=stats)\n",
    "\n",
    "for x, y in a:\n",
    "    print(x.shape, y.shape)\n",
    "    break\n",
    "\n",
    "a.on_epoch_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 43200) (43200,) (100,) ()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.58"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# sanity check the generator output\n",
    "train_batch_generator = SklearnBatchGenerator(x_train, y_train, batch_size, AUGMENTATIONS_TRAIN, stats)\n",
    "valid_batch_generator = SklearnBatchGenerator(x_valid, y_valid, batch_size, AUGMENTATIONS_TEST, stats)\n",
    "\n",
    "train_batch_generator.on_epoch_end()\n",
    "valid_batch_generator.on_epoch_end()\n",
    "\n",
    "clf = LogisticRegression()\n",
    "x, y = train_batch_generator[0]\n",
    "print(x.shape, x[0].shape, y.shape, y[0].shape)\n",
    "clf.fit(x, y)\n",
    "\n",
    "x, y = valid_batch_generator[0]\n",
    "pred = clf.predict(x)\n",
    "accuracy_score(pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def get_model_and_metadata_from_gcs(bucket, model_dir, model_file_ext, model_load_func, gcs_model_dir, experiment_name):\n",
    "    model_and_metadata_filepath = os.path.join(model_dir, experiment_name)\n",
    "    metadata_filepath = f\"{model_and_metadata_filepath}_metadata.json\"\n",
    "    model_filepath = f\"{model_and_metadata_filepath}.{model_file_ext}\"\n",
    "    \n",
    "    gcs_model_and_metadata_filepath = os.path.join(gcs_model_dir, experiment_name)\n",
    "    gcs_metadata_filepath = f\"{gcs_model_and_metadata_filepath}_metadata.json\"\n",
    "    gcs_model_filepath = f\"{gcs_model_and_metadata_filepath}.{model_file_ext}\"\n",
    "    \n",
    "    print(gcs_model_filepath)\n",
    "\n",
    "    gcs_metadata_blob = bucket.blob(gcs_metadata_filepath)\n",
    "    gcs_model_blob = bucket.blob(gcs_model_filepath)\n",
    "    \n",
    "    if gcs_metadata_blob.exists():\n",
    "        print('Loading previously trained model.')\n",
    "        gcs_metadata_blob.download_to_filename(metadata_filepath)\n",
    "\n",
    "        with open(metadata_filepath, 'r') as json_file:\n",
    "            model_metadata = json.load(json_file)\n",
    "\n",
    "        gcs_model_blob.download_to_filename(model_filepath)\n",
    "\n",
    "        model = model_load_func(model_filepath)\n",
    "        return model, model_metadata\n",
    "\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/models/sgd_classifier_default_2020_1_31.joblib\n",
      "Loading previously trained model.\n",
      "Resuming training at epoch 5\n",
      "epoch_num 6 - 2.6665 sec - 0.58\n",
      "epoch_num 7 - 1.7891 sec - 0.58\n",
      "epoch_num 8 - 2.0057 sec - 0.58\n",
      "epoch_num 9 - 2.1827 sec - 0.58\n",
      "training completed in 1.1429970264434814 seconds\n",
      "prediction completed in 2.1457488536834717 seconds\n",
      "epoch_num 10 - 2.1466 sec - 0.58\n",
      "epoch_num 11 - 2.0036 sec - 0.58\n",
      "epoch_num 12 - 3.6074 sec - 0.58\n",
      "Ending training due to no improvement\n"
     ]
    }
   ],
   "source": [
    "import joblib \n",
    "\n",
    "history = list()\n",
    "\n",
    "train_batch_generator = SklearnBatchGenerator(x_train, y_train, batch_size, AUGMENTATIONS_TRAIN, stats)\n",
    "valid_batch_generator = SklearnBatchGenerator(x_valid, y_valid, batch_size, AUGMENTATIONS_TEST, stats)\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "now = datetime.datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "experiment_name = f\"sgd_classifier_default_2020_1_31\"\n",
    "gcs_model_dir = \"model/models\"\n",
    "model_path = os.path.join(model_dir, experiment_name + \".joblib\")\n",
    "model_gcs_path = os.path.join(gcs_model_dir, experiment_name + \".joblib\")\n",
    "model_metadata_path = os.path.join(model_dir, experiment_name + \"_metadata.json\")\n",
    "model_metadata_gcs_path = os.path.join(gcs_model_dir, experiment_name + \"_metadata.json\")\n",
    "\n",
    "model, model_base_metadata = get_model_and_metadata_from_gcs(bucket, model_dir, \"joblib\", joblib.load, gcs_model_dir, \n",
    "                                                             experiment_name)\n",
    "\n",
    "if model is not None:\n",
    "    print('Resuming training at epoch', model_base_metadata['epoch'])\n",
    "else:\n",
    "    model = SGDClassifier()\n",
    "    model_base_metadata = {\n",
    "        'data': 'train_valid_google_automl_cloud_and_shadow_dataset_small.csv',\n",
    "        'data_prep': 'normalization_augmentation',\n",
    "        'experiment_name': experiment_name,\n",
    "        'experiment_start_time': now,\n",
    "        'model': SGDClassifier.__name__,\n",
    "        'random_state': random_seed\n",
    "    }\n",
    "        \n",
    "# Shuffle the data\n",
    "train_batch_generator.on_epoch_end()\n",
    "valid_batch_generator.on_epoch_end()\n",
    "train_start = time.time()\n",
    "best_model = None\n",
    "for epoch in range(int(model_base_metadata['epoch']) + 1, n_epochs):\n",
    "    start = time.time()\n",
    "    for batch_x, batch_y in train_batch_generator.make_one_shot_iterator():\n",
    "        model.partial_fit(batch_x, batch_y, classes=classes)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"training completed in\", time.time() - start, \"seconds\")\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    actual_y_train, pred_y_train = train_batch_generator.get_predictions(clf)\n",
    "    actual_y_valid, pred_y_valid = valid_batch_generator.get_predictions(clf)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"prediction completed in\", time.time() - start, \"seconds\")\n",
    "\n",
    "    epoch_time = f\"{time.time() - start:.4f}\"\n",
    "    epoch_metrics = {\n",
    "        'accuracy_train': sklearn.metrics.accuracy_score(actual_y_train, pred_y_train),\n",
    "        'accuracy_valid': sklearn.metrics.accuracy_score(actual_y_valid, pred_y_valid),        \n",
    "        \"f1_score_train\": sklearn.metrics.f1_score(actual_y_train, pred_y_train),\n",
    "        \"f1_score_valid\": sklearn.metrics.f1_score(actual_y_valid, pred_y_valid),        \n",
    "    }\n",
    "    history.append(epoch_metrics)\n",
    "    \n",
    "    print(\"epoch_num\", epoch, \"-\", epoch_time, \"sec -\", epoch_metrics['accuracy_valid'])\n",
    "        \n",
    "    if len(history) < 2:\n",
    "        continue\n",
    "        \n",
    "    if epoch_metrics['accuracy_valid'] <= history[-2]['accuracy_valid']:\n",
    "        epochs_without_improvement += 1\n",
    "    else:\n",
    "        dump(model, model_path)\n",
    "        with open(model_metadata_path, 'w+') as json_file:\n",
    "            model_base_metadata.update({\n",
    "                'epoch': str(epoch),\n",
    "                'confusion_matrix': numpy_to_json(confusion_matrix(actual_y_valid, pred_y_valid)),\n",
    "                'precision_recall_curve': sklearn_precision_recall_curve_to_dict(\n",
    "                    sklearn.metrics.precision_recall_curve(actual_y_valid, pred_y_valid)),\n",
    "                'history': history,\n",
    "                'train_time_elapsed': time.time() - train_start\n",
    "            })\n",
    "            json.dump(model_base_metadata, json_file)\n",
    "        \n",
    "        for filename, gcs_filename in [(model_path, model_gcs_path), (model_metadata_path, model_metadata_gcs_path)]:\n",
    "            blob = bucket.blob(gcs_filename)\n",
    "            blob.upload_from_filename(filename)\n",
    "            \n",
    "        epochs_without_improvement = 0\n",
    "    \n",
    "    if epochs_without_improvement == early_stopping_patience:\n",
    "        print(\"Ending training due to no improvement\")\n",
    "        break\n",
    "    \n",
    "    \n",
    "    train_batch_generator.on_epoch_end()\n",
    "    valid_batch_generator.on_epoch_end()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.python.lib.io import file_io\n",
    "\n",
    "\n",
    "class ModelCheckpointGCS(ModelCheckpoint):\n",
    "    \"\"\"\n",
    "    Computes scikit-learn metrics on train and validation data whenever model reaches a new high \"monitor\" value. Saves\n",
    "    model and training metadata to disk and gcs. Assumes GOOGLE_APPLICATION_CREDENTIALS has been set.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filepath, gcs_filepath, gcs_bucket, model_metadata, monitor='val_loss', verbose=0, mode='auto', period=1):\n",
    "        model_filepath = f\"{filepath}.h5\"\n",
    "        super(ModelCheckpointGCS, self).__init__(filepath=model_filepath, monitor=monitor, verbose=verbose,\n",
    "                                                 save_best_only=True, save_weights_only=False,\n",
    "                                                 mode=mode, period=period)\n",
    "        self.model_filepath = model_filepath\n",
    "        self.model_metadata_filepath = f\"{filepath}_metadata.json\"\n",
    "        self.gcs_bucket = gcs_bucket\n",
    "        self.gcs_model_filepath = f\"{gcs_filepath}.h5\"\n",
    "        self.gcs_model_metadata_filepath = f\"{gcs_filepath}_metadata.json\"\n",
    "        self.model_metadata = model_metadata\n",
    "        self.train_start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"\n",
    "        Based on\n",
    "        https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/callbacks.py#L983\n",
    "        :param epoch:\n",
    "        :param logs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        logs = logs or {}\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            warnings.warn('Can save best model only with %s available, '\n",
    "                          'skipping.' % (self.monitor), RuntimeWarning)\n",
    "        else:\n",
    "            if self.monitor_op(current, self.best):\n",
    "                if self.verbose > 0:\n",
    "                    print('Epoch %05d: %s improved from %0.5f to %0.5f,'\n",
    "                          ' saving model to %s'\n",
    "                          % (epoch, self.monitor, self.best,\n",
    "                             current, self.model_filepath))\n",
    "                self.best = current\n",
    "\n",
    "                # Save model\n",
    "                self.model.save(self.model_filepath, overwrite=True)\n",
    "\n",
    "                blob = self.gcs_bucket.blob(self.gcs_model_filepath)\n",
    "                blob.upload_from_filename(self.model_filepath)\n",
    "\n",
    "                self.model_metadata.update({\n",
    "                    'epoch': str(epoch),\n",
    "                    'history': {key: value.astype(np.float64) for key, value in logs.items()},\n",
    "                    'elapsed_train_time': time.time() - self.train_start_time\n",
    "                })\n",
    "                \n",
    "                with open(self.model_metadata_filepath, 'w+') as json_file:\n",
    "                    json.dump(self.model_metadata, json_file)\n",
    "                \n",
    "                blob = self.gcs_bucket.blob(self.gcs_model_metadata_filepath)\n",
    "                blob.upload_from_filename(self.model_metadata_filepath)\n",
    "\n",
    "            else:\n",
    "                if self.verbose > 0:\n",
    "                    print('Epoch %05d: %s did not improve' %\n",
    "                          (epoch, self.monitor))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/models/cloud_and_shadow_basic_cnn_2020_1_31.h5\n",
      "Loading previously trained model.\n",
      "Resuming training at epoch 4\n",
      "len(train): 100\n",
      "len(valid): 100\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1 steps, validate for 1 steps\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 19s 19s/step - loss: 1.5519 - accuracy: 0.0000e+00 - val_loss: 0.6961 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.7837 - accuracy: 0.0000e+00 - val_loss: 0.6553 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.7036 - accuracy: 0.0000e+00 - val_loss: 0.6641 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6635 - accuracy: 0.0000e+00 - val_loss: 0.6949 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6849 - accuracy: 0.0000e+00 - val_loss: 0.6715 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6557 - accuracy: 0.0000e+00 - val_loss: 0.6540 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6487 - accuracy: 0.0000e+00 - val_loss: 0.6436 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M\")\n",
    "experiment_name = f\"{project_name}_basic_cnn_2020_1_31\"\n",
    "\n",
    "model, model_base_metadata = get_model_and_metadata_from_gcs(bucket, model_dir, \"h5\", load_model, gcs_model_dir, experiment_name)\n",
    "\n",
    "model_and_metadata_filepath = os.path.join(model_dir, experiment_name)\n",
    "gcs_model_and_metadata_filepath = os.path.join(gcs_model_dir, experiment_name)\n",
    "\n",
    "if model is not None:\n",
    "    print('Resuming training at epoch', int(model_base_metadata['epoch']) + 1)\n",
    "else:\n",
    "    model = basic_cnn_model((120, 120, 3), n_classes=n_classes)\n",
    "    model_base_metadata = {\n",
    "        'data': 'train_valid_google_automl_cloud_and_shadow_dataset_small.csv',\n",
    "        'data_prep': 'normalization_augmentation',\n",
    "        'experiment_name': experiment_name,\n",
    "        'experiment_start_time': now,\n",
    "        'model': 'keras_cnn',\n",
    "        'random_state': random_seed,\n",
    "        # so that initial_epoch is 0\n",
    "        'epoch': -1\n",
    "    }        \n",
    "\n",
    "print(f'len(train): {len(x_train)}')\n",
    "print(f'len(valid): {len(x_valid)}')\n",
    "\n",
    "histories = []\n",
    "metrics = [Accuracy()]\n",
    "loss = 'binary_crossentropy'\n",
    "metric_to_monitor = 'val_accuracy'\n",
    "\n",
    "optimizer = Adam()\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "verbosity = 0\n",
    "# Generators\n",
    "train_generator = AugmentedImageSequenceFromNpy(x=x_train, y=y_train, batch_size=batch_size,\n",
    "                                                augmentations=AUGMENTATIONS_TRAIN, stats=stats)\n",
    "\n",
    "valid_generator = AugmentedImageSequenceFromNpy(x=x_valid, y=y_valid, batch_size=batch_size,\n",
    "                                                augmentations=AUGMENTATIONS_TEST, stats=stats)\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=metric_to_monitor, patience=early_stopping_patience, verbose=verbosity),\n",
    "    ReduceLROnPlateau(monitor=metric_to_monitor, factor=0.5, patience=early_stopping_patience, min_lr=0.000001),\n",
    "    TensorBoard(log_dir, histogram_freq=1),\n",
    "    ModelCheckpointGCS(filepath=model_and_metadata_filepath, gcs_filepath=gcs_model_and_metadata_filepath, \n",
    "                       gcs_bucket=bucket, model_metadata=model_base_metadata, monitor=metric_to_monitor, \n",
    "                       verbose=verbosity)\n",
    "]\n",
    "\n",
    "history = model.fit(train_generator, initial_epoch=int(model_base_metadata['epoch']) + 1,\n",
    "                              epochs=n_epochs,\n",
    "                              steps_per_epoch=len(train_generator),\n",
    "                              callbacks=callbacks,\n",
    "                              validation_data=valid_generator, validation_steps=len(valid_generator),\n",
    "                              shuffle=True, verbose=1)\n",
    "\n",
    "actual_y_train, pred_y_train = train_generator.get_predictions(model)\n",
    "actual_y_valid, pred_y_valid = valid_generator.get_predictions(model)\n",
    "\n",
    "metadata_filepath = f\"{model_and_metadata_filepath}_metadata.json\"\n",
    "with open(metadata_filepath, 'r') as json_file:\n",
    "    best_model_metadata = json.load(json_file)\n",
    "\n",
    "best_model_metadata.update({\n",
    "    'accuracy_train': sklearn.metrics.accuracy_score(actual_y_train, pred_y_train),\n",
    "    'accuracy_valid': sklearn.metrics.accuracy_score(actual_y_valid, pred_y_valid),\n",
    "    'f1_score_train': sklearn.metrics.f1_score(actual_y_train, pred_y_train),\n",
    "    'f1_score_valid': sklearn.metrics.f1_score(actual_y_valid, pred_y_valid),\n",
    "    'confusion_matrix': numpy_to_json(sklearn.metrics.confusion_matrix(actual_y_valid, pred_y_valid)),\n",
    "    'precision_recall_curve': sklearn_precision_recall_curve_to_dict(\n",
    "        sklearn.metrics.precision_recall_curve(actual_y_valid, pred_y_valid)),\n",
    "})\n",
    "\n",
    "with open(metadata_filepath, 'w+') as json_file:\n",
    "    json.dump(best_model_metadata, json_file)\n",
    "\n",
    "blob = bucket.blob(f\"{gcs_model_and_metadata_filepath}_metadata.json\")\n",
    "blob.upload_from_filename(metadata_filepath)\n",
    "\n",
    "# Attempt to avoid memory leaks\n",
    "del train_generator\n",
    "del valid_generator\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "# if os.environ.get(\"SHOULD_PREDICT\", \"True\") == \"True\":\n",
    "#     pred_test_labels = predict(model=model, weight_dir=model_path, x=x_test, batch_size=batch_size, n_classes=n_classes)\n",
    "#     clf_report = classification_report(y_test_labels, pred_test_labels, target_names=classes)\n",
    "#     print(clf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# test_generator = AugmentedImageSequenceFromNpy(x=x_test, y=None, batch_size=batch_size,\n",
    "#                                                         augmentations=AUGMENTATIONS_TEST)\n",
    "# y_pred = model.predict(test_generator)\n",
    "# y_pred_binary = [0 if pred < .5 else 1 for pred in y_pred]\n",
    "# clf = classification_report(y_test, y_pred_binary,  target_names=['has_clouds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pd.DataFrame(y_pred)[0].unique())\n",
    "# print(pd.DataFrame(y_pred_binary)[0].unique())\n",
    "# pd.DataFrame(y_pred)[0].unique()\n",
    "# pd.DataFrame(y_pred_binary)[0].unique()\n",
    "# full_histories = join_histories(histories)\n",
    "# graph_model_history(full_histories)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
